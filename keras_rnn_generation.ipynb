{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Generating text with recurrent neural networks using Keras</font>\n",
    "by Melissa Roemmele, 7/17/17, roemmele @ usc.edu\n",
    "\n",
    "## <font color='#6629b2'>Overview</font>\n",
    "\n",
    "I am going to show how to build a recurrent neural network (RNN) language model that learns the relation between words in text, using the Keras library for machine learning. I will then show how this model can be used for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Recurrent Neural Networks (RNNs)</font>\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural langugage processing tasks. Here an RNN will be used as a language model, which can predict which word is likely to occur next in a text given the words before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Keras</font>\n",
    "\n",
    "[Keras](https://keras.io/) is a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of [Theano](http://deeplearning.net/software/theano/) or [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "My research is on story generation, so I've selected a dataset of stories as the text to be modeled by the RNN. They come from the [ROCStories](http://cs.rochester.edu/nlp/rocstories/) dataset, which consists of thousands of five-sentence stories about everyday life events. Here the model will observe all five sentences in each story. Then we'll use the trained model to generate the final sentence in a set of stories not observed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function #Python 2/3 compatibility for print statements\n",
    "import pprint #pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"David noticed he had put on a lot of weight recently. He examined his habits to try and figure out the reason. He realized he'd been eating too much fast food lately. He stopped going to burger places and started a vegetarian diet. After a few weeks, he started to feel much better.\",\n",
      " \"Tom had a very short temper. One day a guest made him very angry. He punched a hole in the wall of his house. Tom's guest became afraid and left quickly. Tom sat on his couch filled with regret about his actions.\"]\n"
     ]
    }
   ],
   "source": [
    "'''load the training dataset'''\n",
    "import csv\n",
    "\n",
    "with open('example_train_stories.csv', 'r') as f:\n",
    "    train_stories = [story for story in csv.reader(f)]\n",
    "    \n",
    "#sentences in stories are comma-separated, so join them\n",
    "train_stories = [\" \".join(story) for story in train_stories]\n",
    "pprint.pprint(train_stories[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>\n",
    "\n",
    "The model we'll create is a word-based language model, which means each input unit is a single word (some language models learn subword units like characters). \n",
    "\n",
    "So first we need to tokenize each of the stories into (lowercased) individual words. I'll use Keras' built-in tokenizer here for convenience, but typically I like to use [spacy](https://spacy.io/), a fast and user-friendly library that performs various language processing tasks. \n",
    "\n",
    "A note: Keras' tokenizer does not do the same linguistic processing to separate punctuation from words, for instance, which should be their own tokens. You can see this below from words that end in punctuation like \".\" or \",\".\n",
    "\n",
    "We need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Thus, each tokenized word in the stories is added to the lexicon. We use the fit_on_texts() function to map each word in the stories to a numerical index. When working with large datasets it's common to filter all words occurring less than a certain number of times, and replace them with some generic \"UNKNOWN\" token. Here, because this dataset is small, every word encountered in the stories is added to the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('raining', 533),\n",
      " ('better,', 534),\n",
      " ('better.', 535),\n",
      " ('four', 536),\n",
      " ('portland.', 537),\n",
      " ('protest', 538),\n",
      " ('sleep', 191),\n",
      " ('ocean!', 539),\n",
      " ('party.', 129),\n",
      " ('up.', 270),\n",
      " ('up,', 540),\n",
      " ('electricity', 271),\n",
      " ('up!', 541),\n",
      " ('presents', 272),\n",
      " ('under', 542),\n",
      " ('worth', 543),\n",
      " ('advice.', 544),\n",
      " ('every', 100),\n",
      " ('today.', 273),\n",
      " ('skills', 545)]\n"
     ]
    }
   ],
   "source": [
    "'''make the lexicon'''\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(train_stories) #split stories into words, assign number to each unique word\n",
    "pprint.pprint(list(tokenizer.word_index.items())[:20])\n",
    "\n",
    "import pickle\n",
    "with open('example_tokenizer.pkl', 'wb') as f: #save the tokenizer\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"David noticed he had put on a lot of weight recently. He examined his habits to try and figure out the reason. He realized he'd been eating too much fast food lately. He stopped going to burger places and started a vegetarian diet. After a few weeks, he started to feel much better.\"\n",
      "[291,\n",
      " 189,\n",
      " 4,\n",
      " 13,\n",
      " 164,\n",
      " 16,\n",
      " 3,\n",
      " 91,\n",
      " 11,\n",
      " 1098,\n",
      " 814,\n",
      " 4,\n",
      " 1440,\n",
      " 8,\n",
      " 1095,\n",
      " 2,\n",
      " 125,\n",
      " 6,\n",
      " 824,\n",
      " 36,\n",
      " 1,\n",
      " 1197,\n",
      " 4,\n",
      " 175,\n",
      " 371,\n",
      " 109,\n",
      " 467,\n",
      " 106,\n",
      " 60,\n",
      " 214,\n",
      " 162,\n",
      " 1360,\n",
      " 4,\n",
      " 238,\n",
      " 49,\n",
      " 2,\n",
      " 1052,\n",
      " 1499,\n",
      " 6,\n",
      " 46,\n",
      " 3,\n",
      " 1063,\n",
      " 522,\n",
      " 37,\n",
      " 3,\n",
      " 126,\n",
      " 1165,\n",
      " 4,\n",
      " 46,\n",
      " 2,\n",
      " 235,\n",
      " 60,\n",
      " 535]\n"
     ]
    }
   ],
   "source": [
    "'''convert each story from text to numbers'''\n",
    "\n",
    "train_idxs = tokenizer.texts_to_sequences(train_stories) #transform each word to its numerical index in lexicon\n",
    "pprint.pprint(train_stories[0])\n",
    "pprint.pprint(train_idxs[0]) #show example of encoded story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Creating a matrix</font>\n",
    "\n",
    "Finally, we need to put all the training stories into a single matrix, where each row is a story and each column is a word index in that story. This enables the model to process the stories in batches as opposed to one at a time, which significantly speeds up training. However, each story has a different number of words. So we create a padded matrix equal to the length on the longest story in the training set. For all stories with fewer words, we prepend the row with zeros representing an empty word position. Then we can actually tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix length: 64\n",
      "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "        291,  189,    4,   13,  164,   16,    3,   91,   11, 1098,  814,\n",
      "          4, 1440,    8, 1095,    2,  125,    6,  824,   36,    1, 1197,\n",
      "          4,  175,  371,  109,  467,  106,   60,  214,  162, 1360,    4,\n",
      "        238,   49,    2, 1052, 1499,    6,   46,    3, 1063,  522,   37,\n",
      "          3,  126, 1165,    4,   46,    2,  235,   60,  535], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''create a padded matrix of stories'''\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = max([len(story) for story in train_idxs]) # get length of longest story\n",
    "print(\"matrix length:\", maxlen)\n",
    "\n",
    "train_idxs = pad_sequences(train_idxs, maxlen=maxlen) #keras provides convenient padding function\n",
    "pprint.pprint(train_idxs[0]) #same example story as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Defining the input and output</font>\n",
    "\n",
    "In an RNN language model, the data is set up so that each word in the text is mapped to the word that follows it. In a given story, for each input word x[idx], the output label y[idx] is just x[idx+1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "        291,  189,    4,   13,  164,   16,    3,   91,   11, 1098,  814,\n",
      "          4, 1440,    8, 1095,    2,  125,    6,  824,   36,    1, 1197,\n",
      "          4,  175,  371,  109,  467,  106,   60,  214,  162, 1360,    4,\n",
      "        238,   49,    2, 1052, 1499,    6,   46,    3, 1063,  522,   37,\n",
      "          3,  126, 1165,    4,   46,    2,  235,   60], dtype=int32)\n",
      "y:\n",
      "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  291,\n",
      "        189,    4,   13,  164,   16,    3,   91,   11, 1098,  814,    4,\n",
      "       1440,    8, 1095,    2,  125,    6,  824,   36,    1, 1197,    4,\n",
      "        175,  371,  109,  467,  106,   60,  214,  162, 1360,    4,  238,\n",
      "         49,    2, 1052, 1499,    6,   46,    3, 1063,  522,   37,    3,\n",
      "        126, 1165,    4,   46,    2,  235,   60,  535], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "'''set up the model input and output'''\n",
    "\n",
    "train_x = train_idxs[:, :-1]\n",
    "print(\"x:\")\n",
    "pprint.pprint(train_x[0])\n",
    "    \n",
    "train_y = train_idxs[:, 1:]#, None] #Keras requires extra dim for y: (batch_size, n_timesteps, 1)\n",
    "print(\"y:\")\n",
    "pprint.pprint(train_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Creating the model</font>\n",
    "\n",
    "We'll build an RNN with four layers: \n",
    "1. An input layer that converts word indices into distributed vector representations (embeddings).\n",
    "2. A recurrent hidden layer, the main component of the network. As it observes each word in the story, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the story at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer.\n",
    "3. A second recurrent layer that takes the first as input and operates the same way, since adding more layers generally improves the model.\n",
    "3. A prediction (dense) layer that outputs a probability for each word in the lexicon via the softmax function, where each probability indicates the chance of that word being the next word in the sequence. The model gets feedback during training about what the actual word should be.\n",
    "\n",
    "Of course this is a very simplified explanation of the model, since the focus here is on how to implement it in Keras. For a more thorough explanation of RNNs, see the resources at the bottom of the notebook.\n",
    "\n",
    "For each layer, we need to specify the number of dimensions (units). For the embedding and recurrent layers, this number can be freely defined (it is typically between 50-1000). For the output (prediction) layer, the number of units is equal to the lexicon size, since the model computes a probability distribution for each word in the lexicon. To account for the zeros in the input, we'll add one more dimension so that each word index corresponds to its output dimension, i.e. the predicted probability of word index 1 is at column index 1 (2nd column) in the probability distribution output.\n",
    "\n",
    "When setting up the model, we specify the number of stories in each input batch (batch size) as well as the number of words in each story (n_timesteps). Here, we'll set n_timesteps to be the length of the x and y matrices above.\\**  So the shape of the input to the model is (batch_size, n_timesteps). The embedding layer needs to be told how many unique word indices there are (input_dim=lexicon size + 1, adding one since the 0 index is reserved for padding) so that it can map each word to a vector of size output_dim=n_embedding_nodes. Thus the shape of the embedding layer output will be (batch_size, n_timesteps, n_embedding_nodes).\n",
    "\n",
    "In the recurrent layers, return_sequences=True indicates the hidden state for each word in the story will be returned, as opposed to just the hidden state for the last word. This is necessary for the model to provide an output for each word. The stateful=True setting indicates the RNN will \"remember\" its hidden state until it is explicitly told to forget it via the reset_states() function. This comes into play during the generation stage (or also when n_timesteps is less than the length of x and y\\**), so I will explain this further below.\n",
    "\n",
    "For each word in a story, the prediction layer will output a probability distribution for the next word. To get this sequence of probability distributions rather than just one, we wrap TimeDistributed() class around the Dense layer. The model is trained to maximize the probabilities of the words in the stories, which is what the sparse_categorical_crossentropy loss function does (again, see below for a full explanation of this). \n",
    "\n",
    "One huge benefit of Keras is that it has several optimization algorithms already implemented. I use Adam here, there are several other available including SGD, RMSprop, and Adagrad. You can change other parameters like learning rate and gradient clipping as well.\n",
    "\n",
    "*\\**It is also possible to set n_timesteps to be less than this length and iterate over shorter sequences of words. For example, if we set n_timesteps to 10, the model will slide over each window of 10 words in the stories and perform an update to the parmaters by backpropogating the gradient over these 10 words (for the details of backpropogation, see below). However, we still want the model to \"remember\" everything in the story, not just the previous 10 words, so Keras provides the \"stateful\" option to do this. By setting \"stateful=True\" (here is it False), the hidden state of the model after observing 10 words will be carried over to the next word window. After all the words in a batch of stories have been processed, the reset_states() function can be called to indicate the model should now forget its hidden state and start over with the next batch of stories. You'd need to update the training function below to iterate through a batch of stories by n_timesteps at a time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_rnn(lexicon_size, n_embedding_nodes, n_hidden_nodes, batch_size, n_timesteps):\n",
    "\n",
    "    rnn = Sequential()\n",
    "\n",
    "    #Layer 1\n",
    "    embedding_layer = Embedding(batch_input_shape=(batch_size, n_timesteps),\n",
    "                                input_dim=lexicon_size + 1, #add 1 because word indices start at 1, not 0\n",
    "                                output_dim=n_embedding_nodes, \n",
    "                                mask_zero=True) #mask_zero=True will ignore padding\n",
    "    rnn.add(embedding_layer) #output shape is (batch_size, n_timesteps, n_embedding_nodes)\n",
    "\n",
    "    #Layer 2\n",
    "    recurrent_layer1 = GRU(n_hidden_nodes,\n",
    "                           return_sequences=True, #return hidden state for each word, not just last one\n",
    "                           stateful=True) #keep track of hidden state while iterating through story\n",
    "    rnn.add(recurrent_layer1) #output shape is (batch_size, n_timesteps, n_hidden_nodes)\n",
    "\n",
    "    #Layer 3\n",
    "    recurrent_layer2 = GRU(n_hidden_nodes,\n",
    "                           return_sequences=True, \n",
    "                           stateful=True)\n",
    "    rnn.add(recurrent_layer2)  #output shape is (batch_size, n_timesteps, n_hidden_nodes)\n",
    "\n",
    "    #Layer 4\n",
    "    prediction_layer = TimeDistributed(Dense(lexicon_size + 1,\n",
    "                                       activation=\"softmax\"))\n",
    "    rnn.add(prediction_layer) #output shape is (batch_size, n_timesteps, lexicon_size + 1)\n",
    "\n",
    "    #Specify loss function and optimization algorithm, compile model\n",
    "    rnn.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer='adam')\n",
    "    \n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an RNN with 300 embedding nodes and 500 hidden nodes in each recurrent layer, with a batch size of 20 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''initialize the RNN'''\n",
    "\n",
    "batch_size = 20\n",
    "rnn = create_rnn(lexicon_size = len(tokenizer.word_index),\n",
    "                 n_embedding_nodes = 300,\n",
    "                 n_hidden_nodes = 500,\n",
    "                 batch_size = batch_size,\n",
    "                 n_timesteps = maxlen - 1) #subtract 1 from maxlen because x and y each have one word less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the RNN for 10 iterations through the training stories (epochs). The cross-entropy loss indicates how well the model is learning - it should go down with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN on 100 stories for 10 epochs...\n",
      "epoch 1 mean loss: 7.268\n",
      "epoch 2 mean loss: 6.534\n",
      "epoch 3 mean loss: 6.272\n",
      "epoch 4 mean loss: 6.185\n",
      "epoch 5 mean loss: 6.158\n",
      "epoch 6 mean loss: 6.140\n",
      "epoch 7 mean loss: 6.120\n",
      "epoch 8 mean loss: 6.099\n",
      "epoch 9 mean loss: 6.077\n",
      "epoch 10 mean loss: 6.055\n"
     ]
    }
   ],
   "source": [
    "'''train the RNN'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "n_epochs = 10\n",
    "print(\"Training RNN on\", len(train_stories), \"stories for\", n_epochs, \"epochs...\")\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []  #track cross-entropy loss during training\n",
    "    for batch_idx in range(0, len(train_stories), batch_size):\n",
    "        batch_x = train_x[batch_idx:batch_idx+batch_size] #get batch for x\n",
    "        batch_y = train_y[batch_idx:batch_idx+batch_size, :, None] #Keras requires y shape:(batch_size, y_length, 1)\n",
    "        loss = rnn.train_on_batch(batch_x, batch_y) #takes a few moments to initialize training\n",
    "        losses.append(loss)\n",
    "        rnn.reset_states() #reset hidden state after each batch\n",
    "    print(\"epoch\", epoch + 1, \"mean loss: %.3f\" % numpy.mean(losses))\n",
    "    rnn.save_weights('example_rnn_weights.h5') #save parameters of model after each epoch\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Generating sentences</font>\n",
    "\n",
    "Now that the model is trained, it can be used to generate new text\\**. Here, I'll give the model the first four sentences of a new story and have it generate the fifth sentence. To do this, the model reads the initial story in order to produce a probability distribution for the first word in the fifth sentence. We can sample a word from this probability distribution and add it to the story. We repeat this process, each time generating the next word based on the story so far. We stop generating words either when an end-of-sentence token is generated (e.g. \".\", \"!\", or \"?\"). Of course, you can define any stopping criteria (e.g. a specific number of words). \n",
    "\n",
    "*\\**Since the above code takes awhile to run, here I'm going to load a pre-trained model (rnn_96000.h5 and the accompanying tokenizer_96000.pkl) that was trained on 96,000 stories in this corpus for 25 epochs, with the same model parameters shown above. Obviously you should substitute the file names for your trained model here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''in case training was skipped, load all libaries'''\n",
    "\n",
    "import numpy, pickle, csv\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tokenizer with 67197 words in lexicon\n"
     ]
    }
   ],
   "source": [
    "'''load the tokenizer'''\n",
    "\n",
    "with open('tokenizer_96000.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    print(\"loaded tokenizer with\", len(tokenizer.word_index), \"words in lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY: I decided to clean out all the closets. I got a rag and a trash bag. I went into the closets and picked through the old clothing. After throwing out the old stuff, I started cleaning. \n",
      " [10, 28, 2, 414, 29, 32, 1, 19004, 10, 27, 3, 12772, 7, 3, 1228, 1361, 10, 22, 55, 1, 46423, 7, 238, 166, 1, 159, 4856, 44, 1426, 29, 1, 159, 30253, 10, 69, 4126] \n",
      "\n",
      "GIVEN ENDING: I wiped down all the shelves and doors.\n"
     ]
    }
   ],
   "source": [
    "'''load stories used for generation'''\n",
    "\n",
    "with open('example_test_stories.csv', 'r') as f:\n",
    "    heldout_stories = [story for story in csv.reader(f)]\n",
    "\n",
    "#separate final sentence from first four, which will be used for generate new final sentence\n",
    "heldout_endings = [story[-1] for story in heldout_stories[-10:]]\n",
    "heldout_stories = [\" \".join(story[:-1]) for story in heldout_stories[-10:]]\n",
    "heldout_idxs = tokenizer.texts_to_sequences(heldout_stories)\n",
    "print(\"STORY:\", heldout_stories[0], \"\\n\", heldout_idxs[0], \"\\n\")\n",
    "print(\"GIVEN ENDING:\", heldout_endings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will generate word indices, so we need to map these numbers back to their corresponding strings. We'll reverse the lexicon dictionary to create a lookup table to get each word from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'the'),\n",
      " (2, 'to'),\n",
      " (3, 'a'),\n",
      " (4, 'was'),\n",
      " (5, 'he'),\n",
      " (6, 'she'),\n",
      " (7, 'and'),\n",
      " (8, 'her'),\n",
      " (9, 'his'),\n",
      " (10, 'i'),\n",
      " (11, 'in'),\n",
      " (12, 'it'),\n",
      " (13, 'of'),\n",
      " (14, 'for'),\n",
      " (15, 'had'),\n",
      " (16, 'on'),\n",
      " (17, 'they'),\n",
      " (18, 'at'),\n",
      " (19, 'with'),\n",
      " (20, 'that')]\n"
     ]
    }
   ],
   "source": [
    "'''create lookup table to get string words from their indices'''\n",
    "\n",
    "lexicon_lookup = {index: word for word, index in tokenizer.word_index.items()}\n",
    "eos_tokens = [\".\", \"?\", \"!\"] #specify which characters should indicate the end of a sentence and halt generation\n",
    "\n",
    "pprint.pprint(list(lexicon_lookup.items())[:20]) #print a sample of the lookup table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating, the model predicts one word at a time for a given story, but the trained model expects that batch size = 20 and n_timesteps = 63. The easiest thing to do is to create a new model with the same features as the trained model, but set the batch size = 1 and n_timesteps = 1. Then we just load the parameters (weights) from the trained model into generation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''duplicate the trained RNN but set batch size = 1 and n_timesteps = 1'''\n",
    "\n",
    "generation_rnn = create_rnn(lexicon_size = len(tokenizer.word_index),\n",
    "                            n_embedding_nodes = 300,\n",
    "                            n_hidden_nodes = 500,\n",
    "                            batch_size = 1,\n",
    "                            n_timesteps = 1)\n",
    "generation_rnn.load_weights('rnn_weights_96000.h5') #load weights from trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate through each story and generate an ending for it. For each story, we need to \"load\" its first four sentences into the model. This can be done using predict_on_batch() function, even though the probability distributions returned by this function are not needed when just reading the story. Because we set stateful=True when creating the RNN, Keras will keep track of the hidden state while iterating through each word, so that's why n_timesteps can be set to 1. Once the ending has been generated, we call reset_states() to clear the hidden state so that the next story can be read.\n",
    "\n",
    "Once the final word in the fourth sentence has been read in a given story, then we use the resulting probability distribution to predict the first word in the fifth sentence. We use numpy.random.choice() to select a word according to its probability. We once again call predict_on_batch() to get a probability distribution for the second word and sample from this distribution. We continue doing this until a word that ends with an end-of-sentence puncutation mark has been selected. Then we decode the generated ending into a string and show it next to the ending that was given in the dataset.\n",
    "\n",
    "You can see that the generated endings are generally not as coherent and well-formed as the human-authored endings, but they do capture some components of the story and they are often more entertaining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY: I decided to clean out all the closets. I got a rag and a trash bag. I went into the closets and picked through the old clothing. After throwing out the old stuff, I started cleaning.\n",
      "GIVEN ENDING: I wiped down all the shelves and doors.\n",
      "GENERATED ENDING: everything was the best cooler ever. \n",
      "\n",
      "STORY: Kate and her friends were in line outside a club in Las Vegas. They waited close to an hour. They were finally able to go inside. And Kate stepped on a step and broke her heel.\n",
      "GIVEN ENDING: Her night was already ruined.\n",
      "GENERATED ENDING: she wished she had passed. \n",
      "\n",
      "STORY: I was trying to watch my diet. But my family brought home lots of Mexican food. I couldn't resist and dug right in. I ate tons of steak and rice.\n",
      "GIVEN ENDING: But I was glad that I didn't feel bad afterwards.\n",
      "GENERATED ENDING: everyone had no dice where i wanted them. \n",
      "\n",
      "STORY: Maia is an Army brat, so she moves around a lot. Today, she is starting a new school for the second time this year. She got her schedule and then went to her first class. As she headed to an empty seat, other students watched her.\n",
      "GIVEN ENDING: Maia hated being the new girl at school.\n",
      "GENERATED ENDING: when she returned home, she found the cake on the floor and went a couch. \n",
      "\n",
      "STORY: Jane only had one pair of glasses. And she had broken them. Her mother taped them up and sent her to school. When she entered the classroom everyone stared at her.\n",
      "GIVEN ENDING: They all pointed and laughed as she stood wishing she could disappear.\n",
      "GENERATED ENDING: their mother had to wear a backpack. \n",
      "\n",
      "STORY: The man liked the flavor. He tried to recreate it at home. He could not get the flavor right. He asked the owner of the recipe for help.\n",
      "GIVEN ENDING: The owner of the flavor sold him the recipe.\n",
      "GENERATED ENDING: the man bought a heavy egg. \n",
      "\n",
      "STORY: After my friend's dad's funeral, I got in trouble. The principal said I wasn't allowed to leave school that day. He found out I had my friend sign me out. He told me I was getting detention.\n",
      "GIVEN ENDING: I skipped detention all week.\n",
      "GENERATED ENDING: thankfully i skipped detention. \n",
      "\n",
      "STORY: Janice was out exercising for her big soccer game. She was doing some drills with her legs. While working out and exercising she slips on the grass. She falls down and uses her wrist to break her fall.\n",
      "GIVEN ENDING: She breaks her wrist in the process and goes to the hospital.\n",
      "GENERATED ENDING: thanks to her delight her coffee pays off on herself. \n",
      "\n",
      "STORY: Jamie is an american girl. Jamie wants to get married to a mexican man. Her family assumes it's because the man wants a green card. Jamie insist that she is marrying him out of love.\n",
      "GIVEN ENDING: Jamie gets married and they spent the rest of their lives together.\n",
      "GENERATED ENDING: jamie is happy and rose early to have a time at the restaurant. \n",
      "\n",
      "STORY: The orange fell from the tree. It hit a girl on the head. The girl looked up at the tree. Another orange fell from the tree.\n",
      "GIVEN ENDING: That orange broke her nose.\n",
      "GENERATED ENDING: the farmers moved to the driveway today. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''use RNN to generate new endings for stories'''\n",
    "\n",
    "for story, story_idxs, ending in zip(heldout_stories, heldout_idxs, heldout_endings):\n",
    "    print(\"STORY:\", story)\n",
    "    print(\"GIVEN ENDING:\", ending)\n",
    "    \n",
    "    generated_ending = []\n",
    "    \n",
    "    story_idxs = numpy.array(story_idxs)[None] #format story with shape (1, length)\n",
    "    \n",
    "    for step_idx in range(story_idxs.shape[-1]):\n",
    "        p_next_word = generation_rnn.predict_on_batch(story_idxs[:, step_idx])[0,-1] #load the story; input shape will be (1, 1)\n",
    "\n",
    "    while not generated_ending or lexicon_lookup[next_word][-1] not in eos_tokens: #now start predicting new words\n",
    "        next_word = numpy.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "        generated_ending.append(next_word)\n",
    "        p_next_word = generation_rnn.predict_on_batch(numpy.array(next_word)[None,None])[0,-1]\n",
    "    \n",
    "    generation_rnn.reset_states() #reset hidden state after generating ending\n",
    "    \n",
    "    generated_ending = \" \".join([lexicon_lookup[word] \n",
    "                                 for word in generated_ending]) #decode from numbers back into words\n",
    "    print(\"GENERATED ENDING:\", generated_ending, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "Because it's an amusing task and illustrates the power of RNNs, there are now many tutorials online about text generation with RNNs. This one shows one way to do it in Keras with batch training when the length of the sequences is variable. This also demonstrates how you can input existing text into the RNN and generate a continuation of it.\n",
    "\n",
    "There are many ways this language model can be made to be more sophisticated. Here's a few interesting papers from the NLP community that innovate this basic model for different generation tasks:\n",
    "\n",
    "*Recipe generation:* [Globally Coherent Text Generation with Neural Checklist Models](https://homes.cs.washington.edu/~yejin/Papers/emnlp16_neuralchecklist.pdf). Chlo√© Kiddon, Luke Zettlemoyer, and Yejin Choi. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Emotional text generation:* [Affect-LM: A Neural Language Model for Customizable Affective Text Generation](https://arxiv.org/pdf/1704.06851.pdf). Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer. Annual Meeting of the Association for Computational Linguistics (ACL), 2017.\n",
    "\n",
    "*Poetry generation:* [Generating Topical Poetry](https://www.isi.edu/natural-language/mt/emnlp16-poetry.pdf). Marjan Ghazvininejad, Xing Shi, Yejin Choi, and Kevin Knight. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Dialogue generation:* [A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](http://www-etud.iro.umontreal.ca/~sordonia/pdf/naacl15.pdf). Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie1, Jianfeng Gao, Bill Dolan. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Helpful resources about RNNs for text processing</font>\n",
    "\n",
    "Among the [Theano tutorials](http://deeplearning.net/tutorial/) mentioned above, there are two specifically on RNNs for NLP: [semantic parsing](http://deeplearning.net/tutorial/rnnslu.html#rnnslu) and [sentiment analysis](http://deeplearning.net/tutorial/lstm.html#lstm)\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (same model as shown here, with raw Python code) \n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "This [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTMs work and why they are better than plain RNNs (this explanation also applies to the GRU used here)\n",
    "\n",
    "Another [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) that documents well both the theory of RNNs and their implementation in Python (and if you care to implement the details of the stochastic gradient descent and backprogation through time algorithms, this is very informative)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
