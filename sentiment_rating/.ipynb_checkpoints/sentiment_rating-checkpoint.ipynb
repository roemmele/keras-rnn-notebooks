{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Predicting sentiment ratings with recurrent neural networks using Keras</font>\n",
    "### https://github.com/roemmele/keras-rnn-demo/sentiment-rating\n",
    "by Melissa Roemmele, 10/23/17, roemmele @ ict.usc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Overview</font>\n",
    "\n",
    "I am going to show how to use the Keras library to build a recurrent neural network (RNN) model that predicts sentiment ratings for text sequences. Specifically, the model will predict the ratings associated with movie reviews.\n",
    "\n",
    "### <font color='#6629b2'>Recurrent Neural Networks</font>\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural language processing tasks. At a high level, RNN encode sequences via a set of parameters (weights) that are optimized to predict some output variable. The intention of this tutorial is to demonstrate the code needed to assemble an RNN model using the Keras library, as well as some data processing tools that facilitate building the model. \n",
    "\n",
    "If you understand how to structure the input and output of the model, and know the fundamental concepts in machine learning, then just a high-level understanding of how an RNN works is sufficient for using Keras. You'll see that most of the code here is actually just data manipulation, and I'll visualize each step in this process. I'm focusing on this because when I was first learning about NLP, I felt like I lacked a basic understanding of how to represent and manipulate text data in code, maybe because it's assumed that it's trivial to figure out.\n",
    "\n",
    "Even though it is not my focus here, it is more enlightening to understand the technical details of the RNN itself, and it's necessary if you want to innovate on it. For a better understanding of RNNs and neural networks in general, see the resources at the bottom of the notebook.\n",
    "\n",
    "Here an RNN will be used to encode the text of a movie review, and this representation will be used to predict the numerical rating assigned by the reviewer. The model shown here can be applied to any task where the goal is to predict a numerical score associated with a piece of text. Hopefully you can substitute your own datasets and/or modify the code to adapt it to other tasks.\n",
    "\n",
    "### <font color='#6629b2'>Keras</font>\n",
    "\n",
    "[Keras](https://keras.io/) is a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of [Theano](http://deeplearning.net/software/theano/) or [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "The [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) consists of 50,000 movie reviews from [IMDB](http://www.imdb.com/). The ratings are on a 1-10 scale, but the dataset only contains \"polarized\" reviews: positive reviews with a rating of 7 or higher, and negative reviews with a rating of 4 or lower. There are an equal number of positive and negative reviews. The reviews are divided into train and test sets with 25,000 reviews each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function #Python 2/3 compatibility for print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll load the datasets using the [pandas library](https://pandas.pydata.org/), which is extremely useful for any task involving data storage and manipulation. This library puts a dataset into a readable table format, and makes it easy to retrieve specific columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>\"It appears that many critics find the idea of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>The second attempt by a New York intellectual ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>I don't know who to blame, the timid writers o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>This film is mediocre at best. Angie Harmon is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>The film is bad. There is no other way to say ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review\n",
       "0       3  Story of a man who has unnatural feelings for ...\n",
       "1       4  Airport '77 starts as a brand new luxury 747 p...\n",
       "2       4  This film lacked something I couldn't put my f...\n",
       "3       1  Sorry everyone,,, I know this is supposed to b...\n",
       "4       1  When I was little my parents took me along to ...\n",
       "5       3  \"It appears that many critics find the idea of...\n",
       "6       3  The second attempt by a New York intellectual ...\n",
       "7       4  I don't know who to blame, the timid writers o...\n",
       "8       1  This film is mediocre at best. Angie Harmon is...\n",
       "9       2  The film is bad. There is no other way to say ..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the training dataset'''\n",
    "\n",
    "import pandas\n",
    "\n",
    "# For demo purposes, will load only the first 100 reviews in the training set\n",
    "train_reviews = pandas.read_csv('dataset/imdb_train_reviews.csv', encoding='utf-8')[:100]\n",
    "train_reviews[:10] #Show a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Tokenization</font>\n",
    "\n",
    "The first pre-processing step is to tokenize each of the reviews into (lowercased) individual words, since the RNN will encode the reviews word by word. For this I'll use [spacy](https://spacy.io/), which is a fast and extremely user-friendly library that performs various language processing tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Tokenized_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>[story, of, a, man, who, has, unnatural, feeli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>[airport, ', 77, starts, as, a, brand, new, lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>[this, film, lacked, something, i, could, n't,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>[sorry, everyone, ,, ,, ,, i, know, this, is, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>[when, i, was, little, my, parents, took, me, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"It appears that many critics find the idea of...</td>\n",
       "      <td>[\", it, appears, that, many, critics, find, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The second attempt by a New York intellectual ...</td>\n",
       "      <td>[the, second, attempt, by, a, new, york, intel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don't know who to blame, the timid writers o...</td>\n",
       "      <td>[i, do, n't, know, who, to, blame, ,, the, tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This film is mediocre at best. Angie Harmon is...</td>\n",
       "      <td>[this, film, is, mediocre, at, best, ., angie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The film is bad. There is no other way to say ...</td>\n",
       "      <td>[the, film, is, bad, ., there, is, no, other, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  Story of a man who has unnatural feelings for ...   \n",
       "1  Airport '77 starts as a brand new luxury 747 p...   \n",
       "2  This film lacked something I couldn't put my f...   \n",
       "3  Sorry everyone,,, I know this is supposed to b...   \n",
       "4  When I was little my parents took me along to ...   \n",
       "5  \"It appears that many critics find the idea of...   \n",
       "6  The second attempt by a New York intellectual ...   \n",
       "7  I don't know who to blame, the timid writers o...   \n",
       "8  This film is mediocre at best. Angie Harmon is...   \n",
       "9  The film is bad. There is no other way to say ...   \n",
       "\n",
       "                                    Tokenized_Review  \n",
       "0  [story, of, a, man, who, has, unnatural, feeli...  \n",
       "1  [airport, ', 77, starts, as, a, brand, new, lu...  \n",
       "2  [this, film, lacked, something, i, could, n't,...  \n",
       "3  [sorry, everyone, ,, ,, ,, i, know, this, is, ...  \n",
       "4  [when, i, was, little, my, parents, took, me, ...  \n",
       "5  [\", it, appears, that, many, critics, find, th...  \n",
       "6  [the, second, attempt, by, a, new, york, intel...  \n",
       "7  [i, do, n't, know, who, to, blame, ,, the, tim...  \n",
       "8  [this, film, is, mediocre, at, best, ., angie,...  \n",
       "9  [the, film, is, bad, ., there, is, no, other, ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Split texts into lists of words (tokens)'''\n",
    "\n",
    "import spacy\n",
    "\n",
    "encoder = spacy.load('en')\n",
    "\n",
    "def text_to_tokens(text_seqs):\n",
    "    token_seqs = [[word.lower_ for word in encoder(text_seq)] for text_seq in text_seqs]\n",
    "    return token_seqs\n",
    "\n",
    "train_reviews['Tokenized_Review'] = text_to_tokens(train_reviews['Review'])\n",
    "    \n",
    "train_reviews[['Review','Tokenized_Review']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Lexicon</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Each tokenized word in the reviews is added to the lexicon, and then each word is mapped to a numerical index that can be read by the model. Since large datasets may contain a huge number of unique words, it's common to filter all words occurring less than a certain number of times, and replace them with some generic &lt;UNK&gt; token. The min_freq parameter in the function below defines this threshold. In the example code, the min_freq parameter is set to 1, so the lexicon will contain all unique words in the training set. When assigning the indices, the number 1 will represent unknown words. The number 0 will represent \"empty\" word slots, which is explained below. Therefore \"real\" words will have indices of 2 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('story', 2),\n",
      " ('of', 3),\n",
      " ('a', 4),\n",
      " ('man', 5),\n",
      " ('who', 6),\n",
      " ('has', 7),\n",
      " ('unnatural', 8),\n",
      " ('feelings', 9),\n",
      " ('for', 10),\n",
      " ('pig', 11),\n",
      " ('.', 12),\n",
      " ('starts', 13),\n",
      " ('out', 14),\n",
      " ('with', 15),\n",
      " ('opening', 16),\n",
      " ('scene', 17),\n",
      " ('that', 18),\n",
      " ('is', 19),\n",
      " ('terrific', 20),\n",
      " ('example', 21)]\n",
      "4502 words in lexicon\n"
     ]
    }
   ],
   "source": [
    "'''Count tokens (words) in texts and add them to the lexicon'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 2. 0 is reserved for padding, and 1 for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    pprint.pprint(list(lexicon.items())[:20])\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "lexicon = make_lexicon(token_seqs=train_reviews['Tokenized_Review'], min_freq=10)\n",
    "print(\"{} words in lexicon\".format(len(lexicon)))\n",
    "\n",
    "with open('pretrained_model/lexicon.pkl', 'wb') as f: # Save the lexicon by pickling it\n",
    "    pickle.dump(lexicon, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>From strings to numbers</font>\n",
    "\n",
    "Once the lexicon is built, we can transform each review from string tokens into a list of numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Review</th>\n",
       "      <th>Review_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[story, of, a, man, who, has, unnatural, feeli...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[airport, ', 77, starts, as, a, brand, new, lu...</td>\n",
       "      <td>[93, 94, 95, 12, 96, 3, 97, 98, 99, 100, 101, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[this, film, lacked, something, i, could, n't,...</td>\n",
       "      <td>[218, 398, 473, 344, 234, 300, 236, 474, 256, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[sorry, everyone, ,, ,, ,, i, know, this, is, ...</td>\n",
       "      <td>[527, 528, 30, 30, 30, 234, 501, 218, 18, 529,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[when, i, was, little, my, parents, took, me, ...</td>\n",
       "      <td>[290, 234, 226, 302, 256, 583, 584, 519, 411, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\", it, appears, that, many, critics, find, th...</td>\n",
       "      <td>[530, 37, 734, 17, 381, 735, 685, 34, 299, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[the, second, attempt, by, a, new, york, intel...</td>\n",
       "      <td>[34, 219, 770, 33, 3, 98, 771, 772, 123, 773, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[i, do, n't, know, who, to, blame, ,, the, tim...</td>\n",
       "      <td>[234, 336, 236, 501, 5, 66, 886, 30, 34, 887, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[this, film, is, mediocre, at, best, ., angie,...</td>\n",
       "      <td>[218, 398, 18, 933, 476, 255, 11, 891, 912, 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[the, film, is, bad, ., there, is, no, other, ...</td>\n",
       "      <td>[34, 398, 18, 303, 11, 328, 18, 44, 407, 951, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Tokenized_Review  \\\n",
       "0  [story, of, a, man, who, has, unnatural, feeli...   \n",
       "1  [airport, ', 77, starts, as, a, brand, new, lu...   \n",
       "2  [this, film, lacked, something, i, could, n't,...   \n",
       "3  [sorry, everyone, ,, ,, ,, i, know, this, is, ...   \n",
       "4  [when, i, was, little, my, parents, took, me, ...   \n",
       "5  [\", it, appears, that, many, critics, find, th...   \n",
       "6  [the, second, attempt, by, a, new, york, intel...   \n",
       "7  [i, do, n't, know, who, to, blame, ,, the, tim...   \n",
       "8  [this, film, is, mediocre, at, best, ., angie,...   \n",
       "9  [the, film, is, bad, ., there, is, no, other, ...   \n",
       "\n",
       "                                         Review_Idxs  \n",
       "0  [1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 10, 11, 12, 13,...  \n",
       "1  [93, 94, 95, 12, 96, 3, 97, 98, 99, 100, 101, ...  \n",
       "2  [218, 398, 473, 344, 234, 300, 236, 474, 256, ...  \n",
       "3  [527, 528, 30, 30, 30, 234, 501, 218, 18, 529,...  \n",
       "4  [290, 234, 226, 302, 256, 583, 584, 519, 411, ...  \n",
       "5  [530, 37, 734, 17, 381, 735, 685, 34, 299, 2, ...  \n",
       "6  [34, 219, 770, 33, 3, 98, 771, 772, 123, 773, ...  \n",
       "7  [234, 336, 236, 501, 5, 66, 886, 30, 34, 887, ...  \n",
       "8  [218, 398, 18, 933, 476, 255, 11, 891, 912, 18...  \n",
       "9  [34, 398, 18, 303, 11, 328, 18, 44, 407, 951, ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Convert each text from a list of tokens to a list of numbers (indices)'''\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_reviews['Review_Idxs'] = tokens_to_idxs(token_seqs=train_reviews['Tokenized_Review'], \n",
    "                                              lexicon=lexicon)\n",
    "                                   \n",
    "train_reviews[['Tokenized_Review', 'Review_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Numerical lists to matrices</font>\n",
    "\n",
    "We need to put all the reviews in the training set into a single matrix, where each row is a review and each column is a word index in that sequence. This enables the model to process multiple sequences in parallel (batches) as opposed to one at a time. Using batches significantly speeds up training. However, each review has a different number of words, so we create a padded matrix equal to the length on the longest review in the training set. For all reviews with fewer words, we prepend the row with zeros representing an empty word position. We can tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of input matrix: 1184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   91,   92,   11],\n",
       "       [   0,    0,    0, ...,  472,  116,   11],\n",
       "       [   0,    0,    0, ...,    9,  526,   11],\n",
       "       ..., \n",
       "       [   0,    0,    0, ...,  782,  782,  782],\n",
       "       [   0,    0,    0, ...,  121, 4492,   11],\n",
       "       [   0,    0,    0, ...,  252, 4155,   11]], dtype=int32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Create a padded matrix of input reviews'''\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs):\n",
    "    max_seq_len = max([len(idx_seq) for idx_seq in idx_seqs]) # Get length of longest sequence\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len) # Keras provides a convenient padding function\n",
    "    return padded_idxs\n",
    "\n",
    "train_padded_idxs = pad_idx_seqs(train_reviews['Review_Idxs'])\n",
    "print(\"length of input matrix: {}\".format(train_padded_idxs.shape[-1]))\n",
    "\n",
    "train_padded_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Building the model</font>\n",
    "\n",
    "To assemble the model, we'll use Keras' [Functional API](https://keras.io/getting-started/functional-api-guide/), which is one of two ways to use Keras to assemble models (the alternative is the [Sequential API](https://keras.io/getting-started/sequential-model-guide/), which is a bit simpler but has more constraints). A model consists of a series of layers. As shown in the code below, we initialize instances for each layer. Each layer can be called with another layer as input, e.g. Embedding()(input_layer). A model instance is initialized with the Model() object, which defines the initial input and final output layers for that model. Before the model can be trained, the compile() function must be called with the loss function and optimization algorithm specified (see below).\n",
    "\n",
    "###  <font color='#6629b2'>Layers</font>\n",
    "\n",
    "We'll build an RNN with three layers:\n",
    "\n",
    "**1. Embedding**: An input [layer](https://keras.io/layers/embeddings/) that converts word indices into distributed vector representations (embeddings). The mask_zero=True parameter indicates that values of 0 in the matrix (the padding) will be ignored by the model.\n",
    "\n",
    "**2. GRU**: A [recurrent (GRU) hidden layer](https://keras.io/layers/recurrent/), the central component of the model. As it observes each word in the story, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the review at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer (see the materials at the bottom for an explanation of the difference). This layer outputs the last hidden state of the sequence (i.e. the hidden representation of the review after its last word is observed).\n",
    "\n",
    "**3. Dense**: An output [layer](https://keras.io/layers/core/#dense) that predicts the rating for the review based on its GRU representation given by the previous layer. This output is continuous (i.e. ranging from 1-10) rather than categorical. The model gets feedback during training about what the actual rating should be.\n",
    "\n",
    "The term \"layer\" is just an abstraction, when really all these layers are just matrices. The \"weights\" that connect the layers are also matrices. The process of training a neural network is a series of matrix multiplications. The weight matrices are the values that are adjusted during training in order for the model to learn to predict ratings. \n",
    "\n",
    "###  <font color='#6629b2'>Parameters</font>\n",
    "\n",
    "Our function for creating the model takes the following parameters:\n",
    "\n",
    "**n_input_nodes**: the number of unique words in the lexicon, plus one to account for the padding represented by 0 values. This indicates the number of rows in the embedding layer, where each row corresponds to a word.\n",
    "\n",
    "**n_embedding_nodes**: the number of dimensions (units) in the embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the hidden layers. Like the embedding layer, this can be freely chosen. Here, it is set to 500.\n",
    "\n",
    "###  <font color='#6629b2'>Procedure</font>\n",
    "\n",
    "The output of the model is a single continuous value (the predicted rating), making this a regression rather than a classification model. There is only one dimension in the output layer, which contains the predicted rating. Like all neural networks, RNNs learn by updating the parameters (weights) to optimize an objective (loss) function. For this model, the objective is to minimize the mean squared error between the predicted ratings and the actual ratings for the training reviews, thus bringing the predicted ratings closer to the real ratings. The details of this process are extensive; see the resources at the bottom of the notebook if you want a deeper understanding. One huge benefit of Keras is that it implements many of these details for you. Not only does it already have implementations of the types of layer architectures, it also has many of the [loss functions](https://keras.io/losses/) and [optimization methods](https://keras.io/optimizers/) you need for training various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "# from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_model(n_input_nodes, n_embedding_nodes, n_hidden_nodes):\n",
    "    \n",
    "    # Layer 1\n",
    "    input_layer = Input(shape=(None,)) #Length of input matrix will be inferred from input, so None can be given as placeholder\n",
    "    \n",
    "    # Layer 2\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes,\n",
    "                                output_dim=n_embedding_nodes,\n",
    "                                mask_zero=True)(input_layer)\n",
    "    \n",
    "    # Layer 3\n",
    "    gru_layer = GRU(units=n_hidden_nodes)(embedding_layer)\n",
    "    \n",
    "    #Layer 4\n",
    "    output_layer = Dense(units=1)(gru_layer)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[input_layer], outputs=output_layer)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(n_input_nodes=len(lexicon) + 1, n_embedding_nodes=300, n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Training</font>\n",
    "\n",
    "Now we're ready to train the model. Keras' training function lets us specify the batch size and number of times to iterate through the training data (epochs). Keras reports the mean squared error loss after each epoch - if the model is learning correctly, it should progressively decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 49s - loss: 6.0789     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-b799f2295280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'''Train the model'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_padded_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example_model/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#save parameters of model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/Users/roemmele/.theano/compiledir_Darwin-15.6.0-x86_64-i386-64bit-i386-3.6.0-64/scan_perform/mod.cpp:6946)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/theano/tensor/type.py\u001b[0m in \u001b[0;36mvalue_zeros\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \"\"\"\n\u001b[1;32m    553\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "model.fit(x=train_padded_idxs, y=train_reviews['Rating'], batch_size=50, epochs=10)\n",
    "model.save('pretrained_model/model.h5') #save parameters of model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Predicting ratings for reviews</font>\n",
    "\n",
    "Once the model is trained, we can use it predict the ratings for the reviews in the test set. To demonstrate this, I'll load a saved model previously trained on all the reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'example_model/model.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-36fb7b17dd5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load RNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example_model/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;31m# instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/roemmele/miniconda2/envs/python3/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/_objects.c:2846)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/_objects.c:2804)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/Users/ilan/minonda/conda-bld/h5py_1496887972496/work/h5py/h5f.c:2123)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'example_model/model.h5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "'''Load saved model'''\n",
    "\n",
    "# Load lexicon\n",
    "with open('pretrained_model/lexicon.pkl', 'rb') as f:\n",
    "    lexicon = pickle.load(f)\n",
    "\n",
    "# Load RNN model\n",
    "from keras.models import load_model\n",
    "model = load_model('pretrained_model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Load the test dataset, tokenize, and transform to numerical matrix'''\n",
    "\n",
    "test_reviews = pandas.read_csv('dataset/imdb_test_reviews.csv', encoding='utf-8')[:100]\n",
    "test_reviews['Tokenized_Review'] = text_to_tokens(test_reviews['Review'])\n",
    "test_reviews['Review_Idxs'] = tokens_to_idxs(token_seqs=test_reviews['Tokenized_Review'],\n",
    "                                             lexicon=lexicon)\n",
    "test_padded_idxs = pad_idx_seqs(test_reviews['Review_Idxs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can call the predict() function on the test reviews to get the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Pred_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A funny thing happened to me while watching \"M...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This German horror film has to be one of the w...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Being a long-time fan of Japanese film, I expe...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Tokyo Eyes\" tells of a 17 year old Japanese g...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wealthy horse ranchers in Buenos Aires have a ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cage plays a drunk and gets high critically pr...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>First of all, I would like to say that I am a ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>So tell me - what serious boozer drinks Budwei...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A big disappointment for what was touted as an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This film is absolutely appalling and awful. I...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Here's a decidedly average Italian post apocal...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>At the bottom end of the apocalypse movie scal...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Earth has been destroyed in a nuclear holocaus...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Many people are standing in front of the house...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>New York family is the last in their neighborh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The best thing about \"The Prey\" is the tag lin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>This is truly, without exaggerating, one of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I'm a huge fan of both Emily Watson (Breaking ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sure, most of the slasher films of the 1980's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I think that would have been a more appropriat...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1980 was certainly a year for bad backwoods sl...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Everything everyone has said already pretty mu...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Uhhh ... so, did they even have writers for th...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Oh yeah, this one is definitely a strong conte...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Supercraptastic slasher fare, which feels over...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Producers Golan and Globus should have been as...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Aya! If you are looking for special effects th...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The Adventures of Hercules has to be one of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Looking for a REAL super bad movie? If you wan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>From the fertile imagination which brought you...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Although coming after three Star Wars, Krull &amp;...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>JURASSIC PARK III *___ Adventure   Sam Nell ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>I would put Death Wish 3 in the same box as St...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>JP3 lacks the Spielberg touch. It's an all-out...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Did people expect \"Jurassic Park 3\" to be full...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>This movie was perhaps the biggest waste of 2 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>How sad it is when a film as wonderful as \"Jur...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Number 1 was really great summer popcorn fun. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>I had been looking forward to this movie since...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>To summarize, my group of friends and I spent ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>(A possible minor spoiler) The first \"Jurassic...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>It's clear that for this film they wanted to h...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>This would probably be a good film to see....p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>I was disappointed with the third film in the ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>You could have put the characters on the islan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>When i went to see this i thought, i liked the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>How could anyone who liked the previous JP mov...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>I would like to comment on how the girls are c...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Tyra &amp; the rest of the modeling world needs to...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>In watching this off and on for a few seasons,...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Tyra Banks needs to teach these girls that it'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>This is by far the most vapid, idiotic, insane...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>It was awful plain and simple. What was their ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Wow! i think they made this movie to torture p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>This movie could very well have been a propaga...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Rating  Pred_Rating\n",
       "0   Once again Mr. Costner has dragged out a movie...       2            0\n",
       "1   This is an example of why the majority of acti...       4            0\n",
       "2   First of all I hate those moronic rappers, who...       1            0\n",
       "3   Not even the Beatles could write songs everyon...       3            0\n",
       "4   Brass pictures (movies is not a fitting word f...       3            0\n",
       "5   A funny thing happened to me while watching \"M...       2            0\n",
       "6   This German horror film has to be one of the w...       2            0\n",
       "7   Being a long-time fan of Japanese film, I expe...       2            0\n",
       "8   \"Tokyo Eyes\" tells of a 17 year old Japanese g...       4            0\n",
       "9   Wealthy horse ranchers in Buenos Aires have a ...       4            0\n",
       "10  Cage plays a drunk and gets high critically pr...       3            0\n",
       "11  First of all, I would like to say that I am a ...       3            0\n",
       "12  So tell me - what serious boozer drinks Budwei...       2            0\n",
       "13  A big disappointment for what was touted as an...       1            0\n",
       "14  This film is absolutely appalling and awful. I...       1            0\n",
       "15  Here's a decidedly average Italian post apocal...       4            0\n",
       "16  At the bottom end of the apocalypse movie scal...       2            0\n",
       "17  Earth has been destroyed in a nuclear holocaus...       4            0\n",
       "18  Many people are standing in front of the house...       3            0\n",
       "19  New York family is the last in their neighborh...       1            0\n",
       "20  The best thing about \"The Prey\" is the tag lin...       1            0\n",
       "21  This is truly, without exaggerating, one of th...       1            0\n",
       "22  I'm a huge fan of both Emily Watson (Breaking ...       4            0\n",
       "23  Sure, most of the slasher films of the 1980's ...       1            0\n",
       "24  I think that would have been a more appropriat...       3            0\n",
       "25  1980 was certainly a year for bad backwoods sl...       4            0\n",
       "26  Everything everyone has said already pretty mu...       4            0\n",
       "27  Uhhh ... so, did they even have writers for th...       3            0\n",
       "28  Oh yeah, this one is definitely a strong conte...       2            0\n",
       "29  Supercraptastic slasher fare, which feels over...       3            0\n",
       "..                                                ...     ...          ...\n",
       "70  Producers Golan and Globus should have been as...       1            0\n",
       "71  Aya! If you are looking for special effects th...       2            0\n",
       "72  The Adventures of Hercules has to be one of th...       1            0\n",
       "73  Looking for a REAL super bad movie? If you wan...       1            0\n",
       "74  From the fertile imagination which brought you...       4            0\n",
       "75  Although coming after three Star Wars, Krull &...       3            0\n",
       "76    JURASSIC PARK III *___ Adventure   Sam Nell ...       3            0\n",
       "77  I would put Death Wish 3 in the same box as St...       3            0\n",
       "78  JP3 lacks the Spielberg touch. It's an all-out...       3            0\n",
       "79  Did people expect \"Jurassic Park 3\" to be full...       2            0\n",
       "80  This movie was perhaps the biggest waste of 2 ...       1            0\n",
       "81  How sad it is when a film as wonderful as \"Jur...       3            0\n",
       "82  Number 1 was really great summer popcorn fun. ...       3            0\n",
       "83  I had been looking forward to this movie since...       3            0\n",
       "84  To summarize, my group of friends and I spent ...       3            0\n",
       "85  (A possible minor spoiler) The first \"Jurassic...       3            0\n",
       "86  It's clear that for this film they wanted to h...       3            0\n",
       "87  This would probably be a good film to see....p...       1            0\n",
       "88  I was disappointed with the third film in the ...       4            0\n",
       "89  You could have put the characters on the islan...       1            0\n",
       "90  When i went to see this i thought, i liked the...       1            0\n",
       "91  How could anyone who liked the previous JP mov...       1            0\n",
       "92  I would like to comment on how the girls are c...       2            0\n",
       "93  Tyra & the rest of the modeling world needs to...       3            0\n",
       "94  In watching this off and on for a few seasons,...       2            0\n",
       "95  Tyra Banks needs to teach these girls that it'...       1            0\n",
       "96  This is by far the most vapid, idiotic, insane...       1            0\n",
       "97  It was awful plain and simple. What was their ...       1            0\n",
       "98  Wow! i think they made this movie to torture p...       1            0\n",
       "99  This movie could very well have been a propaga...       2            0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Show predicted ratings for test reviews'''\n",
    "\n",
    "#Since ratings are integers, need to round predicted rating to nearest integer\n",
    "test_reviews['Pred_Rating'] = numpy.round(model.predict(test_padded_idxs)[:,0]).astype(int)\n",
    "test_reviews[['Review', 'Rating', 'Pred_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Visualizing inner layers</font>\n",
    "\n",
    "To help visualize the data representation inside the model, we can look at the output of each layer individually. Keras' Functional API lets you derive a new model with the layers from an existing model, so you can define the output to be a layer below the output layer in the original model. Calling predict() on this new model will produce the output of that layer for a given input. Of course, glancing at the numbers by themselves doesn't provide any interpretation of what the model has learned (although there are opportunities to [interpret these values](https://www.civisanalytics.com/blog/interpreting-visualizing-neural-networks-text-processing/)), but seeing them verifies the model is just a series of transformations from one matrix to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD EMBEDDINGS OUTPUT SHAPE: (100, 997, 300)\n",
      "[[-0.0353467   0.01976646  0.00511308 ...,  0.03791321  0.04528227\n",
      "  -0.03195968]\n",
      " [-0.0353467   0.01976646  0.00511308 ...,  0.03791321  0.04528227\n",
      "  -0.03195968]\n",
      " [-0.0353467   0.01976646  0.00511308 ...,  0.03791321  0.04528227\n",
      "  -0.03195968]\n",
      " ..., \n",
      " [ 0.01462577 -0.00791909 -0.03840685 ..., -0.00711416 -0.04995343\n",
      "   0.04736965]\n",
      " [-0.01298066  0.04553379  0.02325361 ...,  0.00554251 -0.00088816\n",
      "  -0.03160277]\n",
      " [-0.00299893  0.01783754 -0.02797754 ...,  0.0101942   0.00263543\n",
      "  -0.03414771]]\n"
     ]
    }
   ],
   "source": [
    "'''Show the output of embedding layer'''\n",
    "\n",
    "embedding_layer = Model(inputs=model.layers[0].input, outputs=model.layers[1].output)\n",
    "embedding_output = embedding_layer.predict(test_padded_idxs)\n",
    "print(\"WORD EMBEDDINGS OUTPUT SHAPE:\", embedding_output.shape)\n",
    "print(embedding_output[0]) # Print embedding vectors for first review in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER OUTPUT SHAPE: (100, 500)\n",
      "[  1.44327991e-02   1.52894668e-02  -3.79302260e-03  -9.19278525e-03\n",
      "  -1.36766676e-02   1.44923721e-02   1.24733448e-02   8.95509729e-05\n",
      "  -1.72275305e-02  -1.01523437e-02   1.83119308e-02   1.23699727e-02\n",
      "   2.48505287e-02   1.88995786e-02   1.91729143e-02  -8.22975300e-04\n",
      "  -2.85958294e-02   6.23047398e-03   1.74080357e-02  -2.74621062e-02\n",
      "  -1.50483353e-02  -9.75272153e-03  -1.10761691e-02  -7.21869757e-03\n",
      "   8.67546257e-03  -1.72035731e-02   5.14779426e-03   9.76586714e-03\n",
      "  -2.35125367e-02   1.70031823e-02  -3.46578076e-04  -1.12660592e-02\n",
      "  -1.84381790e-02   2.26618163e-03  -2.51729181e-03  -4.79329238e-03\n",
      "   1.54964812e-02   1.39020039e-02   1.02428980e-02   2.02537309e-02\n",
      "   1.30586037e-02  -5.97331487e-03   5.82121592e-03  -1.68010872e-02\n",
      "   2.88012624e-03  -2.96132220e-03  -2.01495718e-02  -1.22116413e-03\n",
      "   9.68682580e-03   2.31747068e-02  -1.86998360e-02  -3.17223184e-02\n",
      "  -1.44440513e-02   2.99073998e-02  -2.76044663e-02   1.43079655e-02\n",
      "   6.72551105e-03   1.54770706e-02  -2.31792238e-02   2.44083488e-03\n",
      "  -6.12066220e-03  -5.01957443e-03   7.20797805e-03  -4.46838839e-03\n",
      "  -1.36509929e-02   4.26470209e-03   4.98114061e-03  -1.80038214e-02\n",
      "  -2.29321723e-03   2.11373786e-03   1.43185388e-02  -9.63892788e-04\n",
      "   2.00280827e-02   9.92249697e-03   7.08749751e-03   2.93697845e-02\n",
      "   2.58041285e-02   1.08304694e-02  -1.43676121e-02   4.37923986e-03\n",
      "   2.44092811e-02  -5.36040403e-04   1.08075673e-02  -2.21694224e-02\n",
      "   8.82068276e-03   2.12293975e-02  -1.06064584e-02   1.07844947e-02\n",
      "  -2.83662369e-03   1.43147120e-02   1.16274673e-02   2.09282711e-02\n",
      "   5.66733209e-03  -6.22184668e-03  -1.42321456e-02  -1.25399549e-02\n",
      "  -6.39037555e-03   3.06498017e-02   1.52983144e-02   8.40242766e-03\n",
      "  -9.28877015e-03   1.03893736e-02   2.23665312e-02  -4.73976543e-05\n",
      "   1.17340852e-02  -3.65128089e-03   1.06726522e-02  -2.56008878e-02\n",
      "   1.23199145e-03  -8.47176649e-04  -2.14008428e-03   1.44978184e-02\n",
      "  -1.01878680e-02   6.70932001e-03  -1.10808965e-02  -1.32588279e-02\n",
      "  -2.14593597e-02   7.06486404e-04   2.23397911e-02   1.52150076e-02\n",
      "   1.23406416e-02   6.95236353e-03  -3.43740135e-02  -4.65850066e-03\n",
      "  -2.70360615e-05   7.90022407e-03  -8.71475693e-03   2.33924836e-02\n",
      "  -6.94612041e-03   1.36635490e-02   2.13098638e-02   2.74774618e-04\n",
      "   1.73521470e-02  -1.05599631e-02  -9.11897235e-03  -1.62550379e-02\n",
      "  -1.54648731e-02  -6.04165532e-03  -2.46809237e-03   2.65409425e-02\n",
      "   2.34500039e-04   2.14355439e-02  -4.96415794e-03   7.94474687e-03\n",
      "   6.94233458e-03   1.78542286e-02  -2.53193937e-02   1.28719825e-02\n",
      "   1.07261790e-02   2.66530551e-04  -2.10032091e-02  -1.53609617e-02\n",
      "   1.50447246e-02  -5.56128938e-03  -1.92794576e-02   1.03834989e-02\n",
      "  -1.96007434e-02  -3.26730050e-02   1.61203416e-03  -1.10458825e-02\n",
      "   2.72568199e-03   3.74416634e-03  -6.11669663e-03  -1.70204956e-02\n",
      "  -2.09877137e-02  -1.09630451e-03  -5.45164989e-03   1.14771165e-02\n",
      "  -2.12582061e-04  -1.29341315e-02   1.66963451e-02   1.41724404e-02\n",
      "  -1.86566319e-02  -1.71488747e-02  -1.25369476e-02   1.03874169e-02\n",
      "   6.48940261e-03   7.26385275e-03  -5.80199342e-03   1.71763916e-02\n",
      "  -6.80707768e-03   5.33642806e-03  -9.03133862e-03   2.03577913e-02\n",
      "  -7.04330299e-03  -3.55481636e-03  -9.78513807e-03   1.39495693e-02\n",
      "   1.51122296e-02   2.26376988e-02  -1.41130015e-02   1.66103859e-02\n",
      "  -7.19697401e-03   1.38168968e-03  -1.99889541e-02  -7.19536911e-04\n",
      "   1.41624780e-03   9.37390514e-03   1.55727118e-02  -1.41758416e-02\n",
      "  -1.29339155e-02  -1.42071787e-02   1.97233781e-02   1.57897500e-03\n",
      "   2.81043649e-02  -4.27734153e-03  -2.26434954e-02  -1.27047822e-02\n",
      "   3.14139319e-03  -8.25030077e-03   1.47609059e-02   1.33497529e-02\n",
      "  -2.01338865e-02   1.07704895e-02  -1.06237084e-03  -1.53512415e-02\n",
      "  -8.47447198e-03  -1.29071856e-03   1.10888702e-03   1.23294182e-02\n",
      "   1.27535285e-02  -4.31766640e-03   2.64498289e-03  -9.54056345e-03\n",
      "   3.85473925e-03   7.54658133e-03   4.39749379e-03   8.16511828e-03\n",
      "  -1.58903562e-02   5.88542037e-03   3.93761694e-03  -3.74383200e-03\n",
      "  -7.55977072e-03   1.21227615e-02  -1.87844003e-03   1.32415062e-02\n",
      "   8.54315422e-03  -1.10910311e-02   4.91775665e-03   1.09544396e-02\n",
      "   5.34132635e-03   1.23743145e-02  -1.48574002e-02   5.89840207e-03\n",
      "  -4.56542987e-03   4.69940901e-03   2.64812279e-02   8.78012832e-03\n",
      "  -1.13705741e-02   7.06490362e-04   1.72511265e-02   5.24123479e-03\n",
      "  -1.80353336e-02  -1.32194981e-02   1.90494955e-02  -2.11874992e-02\n",
      "   1.72525470e-03   7.97799230e-03   1.47149209e-02  -1.37297530e-02\n",
      "   3.36123221e-02  -8.23605340e-03  -1.60130172e-03   5.17220516e-03\n",
      "   9.58227087e-03  -1.00450991e-02   6.79200329e-03   7.58233806e-03\n",
      "   2.06804625e-03   4.42025135e-04   3.74412001e-03  -2.01656483e-04\n",
      "  -1.23457741e-02  -8.57955660e-04  -2.67879129e-03  -1.18160574e-02\n",
      "  -4.93471511e-03  -2.36862502e-03  -2.31348947e-02   1.62174590e-02\n",
      "  -6.97419047e-04  -1.77476443e-02   6.18946133e-03   1.80806816e-02\n",
      "  -1.09187211e-03  -2.17486285e-02  -1.24232676e-02   1.68908313e-02\n",
      "  -6.75253011e-03   1.38654616e-02  -1.20369876e-02  -3.58983316e-03\n",
      "  -1.62269361e-02   1.75693203e-02   3.49792652e-02   1.00493366e-02\n",
      "  -1.18880756e-02  -3.81184951e-03  -1.25720445e-02   6.25134166e-03\n",
      "   5.20169269e-05   1.81020284e-03   1.38496570e-02   1.81987626e-03\n",
      "  -9.19122808e-03   3.81829636e-03   1.55866658e-02   1.47823077e-02\n",
      "  -2.59252638e-03  -3.78742069e-03  -1.15117850e-02   1.03849312e-02\n",
      "   1.84345357e-02   1.85420699e-02   1.11480057e-02   1.41117796e-02\n",
      "   1.45718399e-02   1.40948463e-02   5.14921080e-03   2.54510157e-02\n",
      "   1.00497510e-02   3.79327196e-03  -2.15589516e-02  -3.94938737e-02\n",
      "  -6.28593611e-03  -6.08342700e-04   1.49151906e-02  -6.11580908e-03\n",
      "  -1.54690295e-02  -1.45024443e-02  -9.44534037e-03  -1.86627340e-02\n",
      "  -8.87152366e-03  -1.80767309e-02  -7.44903553e-03   1.41986031e-02\n",
      "  -1.00099193e-02  -8.42717942e-03   9.38529521e-03   2.15964764e-02\n",
      "   2.09828187e-03   1.11677237e-02   1.62480976e-02   1.06751695e-02\n",
      "   1.46285258e-02  -1.93882883e-02  -4.47021145e-03  -1.04085095e-02\n",
      "  -2.36031413e-02  -1.04436753e-02  -1.71098374e-02   1.69987325e-04\n",
      "  -1.60835162e-02  -2.26174500e-02  -2.70794146e-02   6.50735805e-03\n",
      "   2.59386823e-02   5.29904431e-03   5.10686915e-03  -7.94238597e-03\n",
      "   1.03371255e-02   2.64168642e-02   1.15526747e-02   7.60114100e-03\n",
      "   6.38699066e-03  -8.08982179e-03  -1.35031305e-02  -9.37761180e-03\n",
      "   3.67341423e-03  -1.88819487e-02  -1.43503696e-02  -4.38410090e-03\n",
      "   1.37606505e-02  -1.35147925e-02   1.89419594e-02  -5.48334233e-03\n",
      "   1.74014755e-02   3.46947787e-03  -2.63910517e-02  -1.36763519e-02\n",
      "   8.82179663e-03  -3.33206961e-03  -1.20609882e-03  -1.78358424e-03\n",
      "   2.45886128e-02  -4.84847091e-03  -2.06652079e-02   6.42253039e-03\n",
      "  -7.78870564e-03   1.28653310e-02  -6.90090284e-03  -1.06075127e-02\n",
      "  -1.23371067e-03   4.58559534e-03   1.96828321e-02   1.58159342e-02\n",
      "   4.14725300e-03  -1.87737569e-02  -2.05947012e-02   2.80603804e-02\n",
      "  -1.98613163e-02   3.05793025e-02   1.31947885e-03   1.40771279e-02\n",
      "   2.64107180e-03  -1.28732249e-03  -2.28803116e-03   7.07969209e-03\n",
      "   1.19157126e-02  -9.85773094e-03  -1.39879752e-02   3.96717563e-02\n",
      "   2.38636211e-02  -1.15868645e-02  -3.48624820e-03   1.30855376e-02\n",
      "   1.64453499e-02   1.26229431e-02   1.88855603e-02  -1.27213821e-02\n",
      "  -9.21281800e-03   1.54748149e-02  -5.48082730e-03   1.47042293e-02\n",
      "   2.46070139e-02   2.13794317e-02   1.08280480e-02  -2.79341675e-02\n",
      "  -2.69475542e-02  -3.17496783e-03  -1.40873063e-02   5.92328096e-03\n",
      "  -1.15134846e-02  -2.73471791e-02  -1.33129414e-02  -1.51307816e-02\n",
      "  -7.52100535e-03  -4.96790651e-03   2.29531694e-02  -9.74594150e-03\n",
      "   1.61065646e-02   2.35689618e-02   2.62163803e-02   4.23727604e-03\n",
      "   1.76144913e-02  -3.69237666e-03   1.27394376e-02  -1.03006661e-02\n",
      "   2.30102465e-02   2.15212349e-03  -1.25340726e-02   1.46014867e-02\n",
      "  -8.69189017e-03   4.81065549e-03   7.84727931e-03  -1.94889940e-02\n",
      "   9.90864541e-03   7.97689334e-03  -8.17321241e-03   4.13152855e-03\n",
      "  -2.42447704e-02   1.64210517e-02   9.99393780e-03  -2.41647232e-02\n",
      "  -2.21783035e-02  -1.63824577e-02  -2.34604813e-02   1.72359473e-03\n",
      "   1.28172627e-02   7.33297365e-03  -4.90933424e-03  -2.20048754e-03\n",
      "   1.62831992e-02  -1.56626031e-02  -3.10423388e-03  -1.00289602e-02\n",
      "  -4.37441515e-03   1.42912718e-03  -4.92556021e-04  -2.10049860e-02\n",
      "  -1.23209311e-02  -1.38195166e-02  -1.03719123e-02   5.39955962e-03\n",
      "  -2.96965223e-02  -4.37593088e-04  -3.44311888e-03  -1.23725999e-02\n",
      "  -2.79158074e-03   8.35409295e-03   1.90022662e-02  -1.86950266e-02\n",
      "   1.98801905e-02  -2.17616651e-02  -3.06448471e-02  -4.98646824e-03\n",
      "   9.43251792e-03  -7.70190870e-03   2.19917968e-02  -1.52422450e-02]\n"
     ]
    }
   ],
   "source": [
    "'''Show the output of recurrent (GRU) layer'''\n",
    "\n",
    "hidden_layer = Model(inputs=model.layers[0].input, outputs=model.layers[2].output)\n",
    "hidden_output = hidden_layer.predict(test_padded_idxs)\n",
    "print(\"HIDDEN LAYER OUTPUT SHAPE:\", hidden_output.shape)\n",
    "print(hidden_output[0]) # Print hidden vector for first review in test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Evaluation</font>\n",
    "\n",
    "A common evaluation for regression models like this one is $R^2$, called the the coefficient of determination. This metric indicates the proportion of variance in the output variable (the rating) that is predictable from the input variable (the review text). The best possible score is 1.0, which indicates the model always predicts the correct rating. The scikit-learn library provides several [evaluation metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) including $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COEFFICIENT OF DETERMINATION (R2): -3.717059\n"
     ]
    }
   ],
   "source": [
    "'''Evaluate the model with R^2'''\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_true=test_reviews['Rating'], y_pred=test_pred_ratings)\n",
    "print(\"COEFFICIENT OF DETERMINATION (R2): {:3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "As mentioned above, the model shown here could be applied to any task where the goal is to predict a score for a particular sequence. For ratings prediction, this score is ordinal, but it could also be categorical with a few simple changes to the output layer of the model. My other tutorials for [language modeling/generation](https://github.com/roemmele/keras-rnn-demo/language-modeling) and [part-of-speech tagging](https://github.com/roemmele/keras-rnn-demo/pos-tagging) demonstrate this type of prediction with categorical variables. They also show how to build an RNN in Keras when the output is a sequence of labels, rather than a single value as shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color='#6629b2'>More resources</font>\n",
    "\n",
    "Yoav Goldberg's book [Neural Network Methods for Natural Language Processing](http://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037) is a thorough introduction to neural networks for NLP tasks in general.\n",
    "\n",
    "If you'd like to learn more about what Keras is doing under the hood, there is a [Theano tutorial](http://deeplearning.net/tutorial/lstm.html) that also applies an RNN to sentiment prediction, using the same dataset here\n",
    "\n",
    "Andrej Karpathy's blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is very helpful for understanding the mathematical details of an RNN, applied to the task of language modeling. It also provides raw Python code with an implementation of the backpropagation algorithm.\n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "Chris Olah provides a good [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTM RNNs work (this explanation also applies to the GRU model used here)\n",
    "\n",
    "Denny Britz's [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) documents well both the technical details of RNNs and their implementation in Python."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
