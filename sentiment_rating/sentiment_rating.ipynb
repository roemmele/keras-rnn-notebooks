{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Predicting sentiment ratings with recurrent neural networks using Keras</font>\n",
    "### https://github.com/roemmele/keras-rnn-demo\n",
    "by Melissa Roemmele, 10/23/17, roemmele @ ict.usc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Overview</font>\n",
    "\n",
    "I am going to show how to use the Keras library to build a recurrent neural network (RNN) model that predicts sentiment ratings for text sequences. Specifically, the model will predict the ratings associated with movie reviews.\n",
    "\n",
    "### <font color='#6629b2'>Recurrent Neural Networks</font>\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural language processing tasks. At a high level, RNN encode sequences via a set of parameters (weights) that are optimized to predict some output variable. The notebook demonstrates the code needed to assemble an RNN model using the Keras library, as well as some data processing tools that facilitate building the model. \n",
    "\n",
    "If you understand how to structure the input and output of the model, and know the fundamental concepts in machine learning, then a high-level understanding of how an RNN works is sufficient for using Keras. You'll see that most of the code here is actually just data manipulation, and I'll visualize each step in this process. The code used to assemble the RNN itself is more minimal. It is of course useful to know these details, so you can theorize on the results and innovate the model to make it better. For a better understanding of RNNs and neural networks in general, see the resources at the bottom of the notebook.\n",
    "\n",
    "Here an RNN will be used to encode the text of a movie review, and this representation will be used to predict the numerical rating assigned by the reviewer. The model shown here can be applied to any task where the goal is to predict a numerical score associated with a piece of text. Hopefully you can substitute your own datasets and/or modify the code to adapt it to other tasks.\n",
    "\n",
    "### <font color='#6629b2'>Keras</font>\n",
    "\n",
    "[Keras](https://keras.io/) is a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of [Theano](http://deeplearning.net/software/theano/) or [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "The [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) consists of 50,000 movie reviews from [IMDB](http://www.imdb.com/). The ratings are on a 1-10 scale, but the dataset only contains \"polarized\" reviews: positive reviews with a rating of 7 or higher, and negative reviews with a rating of 4 or lower. There are an equal number of positive and negative reviews. In the full dataset, the reviews are divided into train and test sets with 25,000 reviews each. Here I'm going to load a sample training set of 100 reviews, so you can download the full dataset at the above link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function #Python 2/3 compatibility for print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll load the datasets using the [pandas library](https://pandas.pydata.org/), which is extremely useful for any task involving data storage and manipulation. This library puts a dataset into a readable table format, and makes it easy to retrieve specific columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>this movie only gets a second star because i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>As I watched this movie, and I began to see it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>this seemed an odd combination of Withnail and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>When I saw the Exterminators of year 3000 at f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>This is a very entertaining flick, considering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Trigger Man\" is definitely the most boring an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>If you havn't seen this movie I highly recomme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>I went to see Fever Pitch with my Mom, and I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>First ever viewing: July 21, 2008  Very impres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>Weak, fast and multicolor,this is the Valvolin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review\n",
       "0       2  this movie only gets a second star because i w...\n",
       "1       8  As I watched this movie, and I began to see it...\n",
       "2       4  this seemed an odd combination of Withnail and...\n",
       "3       9  When I saw the Exterminators of year 3000 at f...\n",
       "4       9  This is a very entertaining flick, considering...\n",
       "5       1  \"Trigger Man\" is definitely the most boring an...\n",
       "6      10  If you havn't seen this movie I highly recomme...\n",
       "7       9  I went to see Fever Pitch with my Mom, and I c...\n",
       "8       9  First ever viewing: July 21, 2008  Very impres...\n",
       "9       7  Weak, fast and multicolor,this is the Valvolin..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the training dataset'''\n",
    "\n",
    "import pandas\n",
    "\n",
    "# For live demo purposes, will load only the first 100 reviews in the training set\n",
    "train_reviews = pandas.read_csv('dataset/example_train_imdb_reviews.csv', encoding='utf-8')\n",
    "train_reviews[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>\n",
    "\n",
    "###  <font color='#6629b2'>Tokenization</font>\n",
    "\n",
    "The first pre-processing step is to tokenize each of the reviews into (lowercased) individual words, since the RNN will encode the reviews word by word. For this I'll use [spaCy](https://spacy.io/), which is a fast and extremely user-friendly library that performs various language processing tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Tokenized_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this movie only gets a second star because i w...</td>\n",
       "      <td>[this, movie, only, gets, a, second, star, bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As I watched this movie, and I began to see it...</td>\n",
       "      <td>[as, i, watched, this, movie, ,, and, i, began...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this seemed an odd combination of Withnail and...</td>\n",
       "      <td>[this, seemed, an, odd, combination, of, withn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When I saw the Exterminators of year 3000 at f...</td>\n",
       "      <td>[when, i, saw, the, exterminators, of, year, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a very entertaining flick, considering...</td>\n",
       "      <td>[this, is, a, very, entertaining, flick, ,, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Trigger Man\" is definitely the most boring an...</td>\n",
       "      <td>[\", trigger, man, \", is, definitely, the, most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you havn't seen this movie I highly recomme...</td>\n",
       "      <td>[if, you, havn't, seen, this, movie, i, highly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I went to see Fever Pitch with my Mom, and I c...</td>\n",
       "      <td>[i, went, to, see, fever, pitch, with, my, mom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>First ever viewing: July 21, 2008  Very impres...</td>\n",
       "      <td>[first, ever, viewing, :, july, 21, ,, 2008,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Weak, fast and multicolor,this is the Valvolin...</td>\n",
       "      <td>[weak, ,, fast, and, multicolor, ,, this, is, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  \\\n",
       "0  this movie only gets a second star because i w...   \n",
       "1  As I watched this movie, and I began to see it...   \n",
       "2  this seemed an odd combination of Withnail and...   \n",
       "3  When I saw the Exterminators of year 3000 at f...   \n",
       "4  This is a very entertaining flick, considering...   \n",
       "5  \"Trigger Man\" is definitely the most boring an...   \n",
       "6  If you havn't seen this movie I highly recomme...   \n",
       "7  I went to see Fever Pitch with my Mom, and I c...   \n",
       "8  First ever viewing: July 21, 2008  Very impres...   \n",
       "9  Weak, fast and multicolor,this is the Valvolin...   \n",
       "\n",
       "                                    Tokenized_Review  \n",
       "0  [this, movie, only, gets, a, second, star, bec...  \n",
       "1  [as, i, watched, this, movie, ,, and, i, began...  \n",
       "2  [this, seemed, an, odd, combination, of, withn...  \n",
       "3  [when, i, saw, the, exterminators, of, year, 3...  \n",
       "4  [this, is, a, very, entertaining, flick, ,, co...  \n",
       "5  [\", trigger, man, \", is, definitely, the, most...  \n",
       "6  [if, you, havn't, seen, this, movie, i, highly...  \n",
       "7  [i, went, to, see, fever, pitch, with, my, mom...  \n",
       "8  [first, ever, viewing, :, july, 21, ,, 2008,  ...  \n",
       "9  [weak, ,, fast, and, multicolor, ,, this, is, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Split texts into lists of words (tokens)'''\n",
    "\n",
    "import spacy\n",
    "\n",
    "encoder = spacy.load('en')\n",
    "\n",
    "def text_to_tokens(text_seqs):\n",
    "    token_seqs = [[word.lower_ for word in encoder(text_seq)] for text_seq in text_seqs]\n",
    "    return token_seqs\n",
    "\n",
    "train_reviews['Tokenized_Review'] = text_to_tokens(train_reviews['Review'])\n",
    "    \n",
    "train_reviews[['Review','Tokenized_Review']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Lexicon</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Each tokenized word in the reviews is added to the lexicon, and then each word is mapped to a numerical index that can be read by the model. Since large datasets may contain a huge number of unique words, it's common to filter all words occurring less than a certain number of times, and replace them with some generic &lt;UNK&gt; token. The min_freq parameter in the function below defines this threshold. When assigning the indices, the number 1 will represent unknown words. The number 0 will represent \"empty\" word slots, which is explained below. Therefore \"real\" words will have indices of 2 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON SAMPLE (2630 total items):\n",
      "[('this', 2), ('movie', 3), ('only', 4), ('gets', 5), ('a', 6), ('second', 7), ('star', 8), ('because', 9), ('i', 10), ('work', 11), ('downtown', 12), ('and', 13), ('liked', 14), ('seeing', 15), ('it', 16), ('destroyed', 17), ('.', 18), ('the', 19), ('effects', 20), ('were', 21)]\n"
     ]
    }
   ],
   "source": [
    "'''Count tokens (words) in texts and add them to the lexicon'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 2. 0 is reserved for padding, and 1 for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(list(lexicon.items())[:20])\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "lexicon = make_lexicon(token_seqs=train_reviews['Tokenized_Review'], min_freq=1)\n",
    "\n",
    "with open('example_model/lexicon.pkl', 'wb') as f: # Save the lexicon by pickling it\n",
    "    pickle.dump(lexicon, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>From strings to numbers</font>\n",
    "\n",
    "Once the lexicon is built, we can use it to transform each review from a list of string tokens into a list of numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Review</th>\n",
       "      <th>Review_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this, movie, only, gets, a, second, star, bec...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[as, i, watched, this, movie, ,, and, i, began...</td>\n",
       "      <td>[112, 10, 113, 2, 3, 51, 13, 10, 114, 74, 115,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[this, seemed, an, odd, combination, of, withn...</td>\n",
       "      <td>[2, 169, 124, 170, 171, 39, 172, 13, 10, 173, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[when, i, saw, the, exterminators, of, year, 3...</td>\n",
       "      <td>[127, 10, 200, 19, 201, 39, 202, 203, 166, 204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[this, is, a, very, entertaining, flick, ,, co...</td>\n",
       "      <td>[2, 77, 6, 137, 263, 266, 51, 267, 19, 268, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[\", trigger, man, \", is, definitely, the, most...</td>\n",
       "      <td>[287, 288, 289, 287, 77, 290, 19, 26, 291, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[if, you, havn't, seen, this, movie, i, highly...</td>\n",
       "      <td>[79, 128, 340, 294, 2, 3, 10, 341, 70, 128, 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[i, went, to, see, fever, pitch, with, my, mom...</td>\n",
       "      <td>[10, 360, 74, 115, 361, 362, 173, 295, 363, 51...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[first, ever, viewing, :, july, 21, ,, 2008,  ...</td>\n",
       "      <td>[204, 30, 397, 237, 398, 399, 51, 400, 301, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[weak, ,, fast, and, multicolor, ,, this, is, ...</td>\n",
       "      <td>[450, 51, 451, 13, 452, 51, 2, 77, 19, 453, 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Tokenized_Review  \\\n",
       "0  [this, movie, only, gets, a, second, star, bec...   \n",
       "1  [as, i, watched, this, movie, ,, and, i, began...   \n",
       "2  [this, seemed, an, odd, combination, of, withn...   \n",
       "3  [when, i, saw, the, exterminators, of, year, 3...   \n",
       "4  [this, is, a, very, entertaining, flick, ,, co...   \n",
       "5  [\", trigger, man, \", is, definitely, the, most...   \n",
       "6  [if, you, havn't, seen, this, movie, i, highly...   \n",
       "7  [i, went, to, see, fever, pitch, with, my, mom...   \n",
       "8  [first, ever, viewing, :, july, 21, ,, 2008,  ...   \n",
       "9  [weak, ,, fast, and, multicolor, ,, this, is, ...   \n",
       "\n",
       "                                         Review_Idxs  \n",
       "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...  \n",
       "1  [112, 10, 113, 2, 3, 51, 13, 10, 114, 74, 115,...  \n",
       "2  [2, 169, 124, 170, 171, 39, 172, 13, 10, 173, ...  \n",
       "3  [127, 10, 200, 19, 201, 39, 202, 203, 166, 204...  \n",
       "4  [2, 77, 6, 137, 263, 266, 51, 267, 19, 268, 13...  \n",
       "5  [287, 288, 289, 287, 77, 290, 19, 26, 291, 13,...  \n",
       "6  [79, 128, 340, 294, 2, 3, 10, 341, 70, 128, 68...  \n",
       "7  [10, 360, 74, 115, 361, 362, 173, 295, 363, 51...  \n",
       "8  [204, 30, 397, 237, 398, 399, 51, 400, 301, 13...  \n",
       "9  [450, 51, 451, 13, 452, 51, 2, 77, 19, 453, 20...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Convert each text from a list of tokens to a list of numbers (indices)'''\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_reviews['Review_Idxs'] = tokens_to_idxs(token_seqs=train_reviews['Tokenized_Review'], \n",
    "                                              lexicon=lexicon)\n",
    "                                   \n",
    "train_reviews[['Tokenized_Review', 'Review_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Numerical lists to matrices</font>\n",
    "\n",
    "We need to put all the reviews in the training set into a single matrix, where each row is a review and each column is a word index in that sequence. This enables the model to process multiple sequences in parallel (batches) as opposed to one at a time. Using batches significantly speeds up training. However, each review has a different number of words, so we create a padded matrix equal to the length on the longest review in the training set. For all reviews with fewer words, we prepend the row with zeros representing an empty word position. We can tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN INPUT:\n",
      " [[   0    0    0 ...,  110  111   97]\n",
      " [   0    0    0 ...,   69  168   18]\n",
      " [   0    0    0 ...,  199   29  176]\n",
      " ..., \n",
      " [   0    0    0 ..., 2599 2337   18]\n",
      " [   0    0    0 ...,   18  301 2609]\n",
      " [   0    0    0 ...,   73  572   18]]\n",
      "SHAPE: (100, 189) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Create a padded matrix of input reviews'''\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs):\n",
    "    max_seq_len = max([len(idx_seq) for idx_seq in idx_seqs]) # Get length of longest sequence\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len) # Keras provides a convenient padding function\n",
    "    return padded_idxs\n",
    "\n",
    "train_padded_idxs = pad_idx_seqs(train_reviews['Review_Idxs'])\n",
    "\n",
    "print(\"TRAIN INPUT:\\n\", train_padded_idxs)\n",
    "print(\"SHAPE:\", train_padded_idxs.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Building the model</font>\n",
    "\n",
    "To assemble the model, we'll use Keras' [Functional API](https://keras.io/getting-started/functional-api-guide/), which is one of two ways to use Keras to assemble models (the alternative is the [Sequential API](https://keras.io/getting-started/sequential-model-guide/), which is a bit simpler but has more constraints). A model consists of a series of layers. As shown in the code below, we initialize instances for each layer. Each layer can be called with another layer as input, e.g. Embedding()(input_layer). A model instance is initialized with the Model() object, which defines the initial input and final output layers for that model. Before the model can be trained, the compile() function must be called with the loss function and optimization algorithm specified (see below).\n",
    "\n",
    "###  <font color='#6629b2'>Layers</font>\n",
    "\n",
    "We'll build an RNN with four layers:\n",
    "\n",
    "**1. Input**: The input layer takes in the matrix of word indices.\n",
    "\n",
    "**2. Embedding**: A [layer](https://keras.io/layers/embeddings/) that converts integer word indices into distributed vector representations (embeddings). The mask_zero=True parameter indicates that values of 0 in the matrix (the padding) will be ignored by the model.\n",
    "\n",
    "**3. GRU**: A [recurrent (GRU) hidden layer](https://keras.io/layers/recurrent/), the central component of the model. As it observes each word in the story, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the review at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer (see the materials at the bottom for an explanation of the difference). This layer outputs the last hidden state of the sequence (i.e. the hidden representation of the review after its last word is observed).\n",
    "\n",
    "**4. Dense**: An output [layer](https://keras.io/layers/core/#dense) that predicts the rating for the review based on its GRU representation given by the previous layer. This output is continuous (i.e. ranging from 1-10) rather than categorical. The model gets feedback during training about what the actual rating should be.\n",
    "\n",
    "The term \"layer\" is just an abstraction, when really all these layers are just matrices. The \"weights\" that connect the layers are also matrices. The process of training a neural network is a series of matrix multiplications. The weight matrices are the values that are adjusted during training in order for the model to learn to predict ratings. \n",
    "\n",
    "###  <font color='#6629b2'>Parameters</font>\n",
    "\n",
    "Our function for creating the model takes the following parameters:\n",
    "\n",
    "**n_input_nodes**: the number of unique words in the lexicon, plus one to account for the padding represented by 0 values. This indicates the number of rows in the embedding layer, where each row corresponds to a word.\n",
    "\n",
    "**n_embedding_nodes**: the number of dimensions (units) in the embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the hidden layers. Like the embedding layer, this can be freely chosen. Here, it is set to 500.\n",
    "\n",
    "###  <font color='#6629b2'>Procedure</font>\n",
    "\n",
    "The output of the model is a single continuous value (the predicted rating), making this a regression rather than a classification model. There is only one dimension in the output layer, which contains the predicted rating. Like all neural networks, RNNs learn by updating the parameters (weights) to optimize an objective (loss) function. For this model, the objective is to minimize the mean squared error between the predicted ratings and the actual ratings for the training reviews, thus bringing the predicted ratings closer to the real ratings. The details of this process are extensive; see the resources at the bottom of the notebook if you want a deeper understanding. One huge benefit of Keras is that it implements many of these details for you. Not only does it already have implementations of the types of layer architectures, it also has many of the [loss functions](https://keras.io/losses/) and [optimization methods](https://keras.io/optimizers/) you need for training various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_model(n_input_nodes, n_embedding_nodes, n_hidden_nodes):\n",
    "    \n",
    "    # Layer 1 -  Technically the shape of this layer is (batch_size, len(train_padded_idxs)).\n",
    "    # However, both the batch size and the length of the input matrix can be inferred from the input at training time. \n",
    "    # The batch size is implictly included in the shape of the input, so it does not need to \n",
    "    # be specified as a dimension of the input. None can be given as placeholder for the input matrix length.\n",
    "    # By defining it as None, the model is flexible in accepting inputs with different lengths.\n",
    "    input_layer = Input(shape=(None,))\n",
    "    \n",
    "    # Layer 2\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes,\n",
    "                                output_dim=n_embedding_nodes,\n",
    "                                mask_zero=True)(input_layer) #mask_zero tells the model to ignore 0 values (padding)\n",
    "    #Output shape = (batch_size, input_matrix_length, n_embedding_nodes)\n",
    "    \n",
    "    # Layer 3\n",
    "    gru_layer = GRU(units=n_hidden_nodes)(embedding_layer)\n",
    "    #Output shape = (batch_size, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    output_layer = Dense(units=1)(gru_layer)\n",
    "    #Output shape = (batch_size, 1)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[input_layer], outputs=output_layer)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model(n_input_nodes=len(lexicon) + 1, n_embedding_nodes=300, n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Training</font>\n",
    "\n",
    "Now we're ready to train the model. Keras will apply batch training by default, even though we didn't specify the batch size when creating the model. If a batch size isn't given, Keras will use its default (32). The training function also indicates the number of times to iterate through the training data (epochs). Keras reports the mean squared error loss after each epoch - if the model is learning correctly, it should progressively decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 5s - loss: 25.5823     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 5s - loss: 23.2152     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 5s - loss: 18.2316     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 5s - loss: 12.7080     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 5s - loss: 10.4225     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "model.fit(x=train_padded_idxs, y=train_reviews['Rating'], batch_size=20, epochs=5)\n",
    "model.save('example_model/model.h5') #save parameters of model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Predicting ratings for reviews</font>\n",
    "\n",
    "Once the model is trained, we can use it predict the ratings for the reviews in the test set. To demonstrate this, I'll load a saved model previously trained on all 25,000 reviews in the training set. I'll apply this model to an example test set of 100 reviews (again, there are 25,000 reviews in the full test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Load saved model'''\n",
    "\n",
    "# Load lexicon\n",
    "with open('pretrained_model/lexicon.pkl', 'rb') as f:\n",
    "    lexicon = pickle.load(f)\n",
    "\n",
    "# Load RNN model\n",
    "from keras.models import load_model\n",
    "model = load_model('pretrained_model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST INPUT:\n",
      " [[    0     0     0 ..., 19451  7875 12041]\n",
      " [    0     0     0 ..., 12884  8579   111]\n",
      " [    0     0     0 ..., 10307 11756   111]\n",
      " ..., \n",
      " [    0     0     0 ..., 13245 19660   111]\n",
      " [    0     0     0 ...,  8195 10307 19831]\n",
      " [    0     0     0 ..., 19799 17344   111]]\n",
      "SHAPE: (100, 577) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Load the test dataset, tokenize, and transform to numerical matrix'''\n",
    "\n",
    "test_reviews = pandas.read_csv('dataset/example_test_imdb_reviews.csv', encoding='utf-8')[:100] #for demo load 100 test reviews\n",
    "test_reviews['Tokenized_Review'] = text_to_tokens(test_reviews['Review'])\n",
    "test_reviews['Review_Idxs'] = tokens_to_idxs(token_seqs=test_reviews['Tokenized_Review'],\n",
    "                                             lexicon=lexicon)\n",
    "test_padded_idxs = pad_idx_seqs(test_reviews['Review_Idxs'])\n",
    "\n",
    "print(\"TEST INPUT:\\n\", test_padded_idxs)\n",
    "print(\"SHAPE:\", test_padded_idxs.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can call the predict() function on the test reviews to get the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Pred_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First of all i'd like to say that this movie i...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible writing, highly contrived, from a \"do...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I didn't expect too much from this movie, but ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Corey Haim is never going to be known as one o...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Being a great fan of Disney, i was really disa...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barbara Payton is the suppose-to-be sultry sex...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Three distinct and distant individuals' lives ...</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I never dreamed when I started watching this D...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Forget Jimmy Stewart reliving his life and opt...</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Have you ever wondered what its like to feel F...</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Glenn Close is back as Sarah Plain and Tall, a...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Directed by the younger brother of great direc...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What could have been some majorly creepy stuff...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>If you've ever listened to any of the James Le...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>THE CELL (2000) Rating: 8/10  The Cell, like A...</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This movie is about a very delicate argument a...</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Captain and Tennille have released a very ...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>As I write this, Norman Wisdom is a very confu...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Well made documentary focusing on two Sudanese...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>An odd, willfully skewed biopic of Dyan Thomas...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>If it is true that the movie only cost 150K to...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I am a Shakespeare lover since childhood. I am...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i have seen this movie about 50 times already ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Most of the critiques on this flick have been ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I love this movie!! Sure I love it because of ...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I've always said that there's nothing to beat ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>This Don Siegel/Clint Eastwood strange and hyp...</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>It's a short movie for such immense feelings. ...</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Okay, I haven't read the book yet but I have t...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Imagine if you will: four teen students have a...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>This film is an absolute disgrace! I thoroughl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Supercarrier was my favorite movie in the late...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Even MST3K couldn't make this painful, long, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>I have viewed this cartoon as a child, a fathe...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Uuuuaaa! Barf! Yuk! Yuk! Disgusting! Puke City...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>A sprawling, overambitious, plotless comedy th...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>I must have been around ten years old when my ...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>I let a friend talk me into viewing this movie...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Low budget horror about an evil force. Hard to...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>I carefully checked if there's another movie n...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>This gem captures early 80's life brilliantly....</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Thank God for the Internet Movie Database!!! W...</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>I'm going to make this short and sweet. It's n...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>With so many good movies coming out in 1995 (p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>I understand that people have different expe...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Well, the movie was no terrible, but whomever ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>I loved that the mood was light and airy. I lo...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>I recently picked up all three Robocop films i...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>The MTV sci-fi animated series \"Ã†on Flux\" is b...</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>I thought the movie was sub-par. The acting wa...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>A cut above from the usual straight to video a...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>After watching this movie on tv, I looked it u...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Let me say from the outset I'm not a particula...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>For those of you who don't remember movies -- ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>The title suggests that this movie is a sequel...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>This is basically just a dumb chase story, nea...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I am completely into this type of story line b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>This movie was outright painful for me to watc...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>The efficacy of this picture was best proven o...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Victor Buono as the Devil? Surely somebody mus...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Rating  Pred_Rating\n",
       "0   First of all i'd like to say that this movie i...      10           10\n",
       "1   Terrible writing, highly contrived, from a \"do...       1            1\n",
       "2   I didn't expect too much from this movie, but ...       4            2\n",
       "3   Corey Haim is never going to be known as one o...       2            1\n",
       "4   Being a great fan of Disney, i was really disa...       3            2\n",
       "5   Barbara Payton is the suppose-to-be sultry sex...       1            3\n",
       "6   Three distinct and distant individuals' lives ...       8            3\n",
       "7   I never dreamed when I started watching this D...      10           10\n",
       "8   Forget Jimmy Stewart reliving his life and opt...       8            7\n",
       "9   Have you ever wondered what its like to feel F...       8           10\n",
       "10  Glenn Close is back as Sarah Plain and Tall, a...      10           10\n",
       "11  Directed by the younger brother of great direc...       7            8\n",
       "12  What could have been some majorly creepy stuff...       4            1\n",
       "13  If you've ever listened to any of the James Le...       4            3\n",
       "14  THE CELL (2000) Rating: 8/10  The Cell, like A...       9            8\n",
       "15  This movie is about a very delicate argument a...       9           10\n",
       "16  The Captain and Tennille have released a very ...       7            9\n",
       "17  As I write this, Norman Wisdom is a very confu...       3            2\n",
       "18  Well made documentary focusing on two Sudanese...       8            9\n",
       "19  An odd, willfully skewed biopic of Dyan Thomas...       4            4\n",
       "20  If it is true that the movie only cost 150K to...       3            2\n",
       "21  I am a Shakespeare lover since childhood. I am...       2            5\n",
       "22  i have seen this movie about 50 times already ...      10           10\n",
       "23  Most of the critiques on this flick have been ...       2            3\n",
       "24  I love this movie!! Sure I love it because of ...      10            9\n",
       "25  I've always said that there's nothing to beat ...       2            1\n",
       "26  This Don Siegel/Clint Eastwood strange and hyp...       7            9\n",
       "27  It's a short movie for such immense feelings. ...       9            5\n",
       "28  Okay, I haven't read the book yet but I have t...       4            4\n",
       "29  Imagine if you will: four teen students have a...       2            2\n",
       "..                                                ...     ...          ...\n",
       "70  This film is an absolute disgrace! I thoroughl...       1            1\n",
       "71  Supercarrier was my favorite movie in the late...      10            9\n",
       "72  Even MST3K couldn't make this painful, long, a...       1            1\n",
       "73  I have viewed this cartoon as a child, a fathe...      10            9\n",
       "74  Uuuuaaa! Barf! Yuk! Yuk! Disgusting! Puke City...       1            1\n",
       "75  A sprawling, overambitious, plotless comedy th...       4            5\n",
       "76  I must have been around ten years old when my ...      10            8\n",
       "77  I let a friend talk me into viewing this movie...       2            1\n",
       "78  Low budget horror about an evil force. Hard to...       1            3\n",
       "79  I carefully checked if there's another movie n...       1            4\n",
       "80  This gem captures early 80's life brilliantly....      10            7\n",
       "81  Thank God for the Internet Movie Database!!! W...      10            8\n",
       "82  I'm going to make this short and sweet. It's n...      10            3\n",
       "83  With so many good movies coming out in 1995 (p...       2            2\n",
       "84    I understand that people have different expe...       2            3\n",
       "85  Well, the movie was no terrible, but whomever ...       4            1\n",
       "86  I loved that the mood was light and airy. I lo...       8            9\n",
       "87  I recently picked up all three Robocop films i...       3            3\n",
       "88  The MTV sci-fi animated series \"Ã†on Flux\" is b...       3            5\n",
       "89  I thought the movie was sub-par. The acting wa...       4            3\n",
       "90  A cut above from the usual straight to video a...       4            6\n",
       "91  After watching this movie on tv, I looked it u...       4            2\n",
       "92  Let me say from the outset I'm not a particula...       7            4\n",
       "93  For those of you who don't remember movies -- ...       1            2\n",
       "94  The title suggests that this movie is a sequel...       7            8\n",
       "95  This is basically just a dumb chase story, nea...       2            3\n",
       "96  I am completely into this type of story line b...       1            1\n",
       "97  This movie was outright painful for me to watc...       2            1\n",
       "98  The efficacy of this picture was best proven o...      10           10\n",
       "99  Victor Buono as the Devil? Surely somebody mus...       3            3\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Show predicted ratings for test reviews'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "#Since ratings are integers, need to round predicted rating to nearest integer\n",
    "test_reviews['Pred_Rating'] = numpy.round(model.predict(test_padded_idxs)[:,0]).astype(int)\n",
    "test_reviews[['Review', 'Rating', 'Pred_Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Evaluation</font>\n",
    "\n",
    "A common evaluation for regression models like this one is $R^2$, called the the coefficient of determination. This metric indicates the proportion of variance in the output variable (the rating) that is predictable from the input variable (the review text). The best possible score is 1.0, which indicates the model always predicts the correct rating. The scikit-learn library provides several [evaluation metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) including $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COEFFICIENT OF DETERMINATION (R2): 0.546074\n"
     ]
    }
   ],
   "source": [
    "'''Evaluate the model with R^2'''\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_true=test_reviews['Rating'], y_pred=test_reviews['Pred_Rating'])\n",
    "print(\"COEFFICIENT OF DETERMINATION (R2): {:3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Visualizing data inside the model</font>\n",
    "\n",
    "To help visualize the data representation inside the model, we can look at the output of each layer individually. Keras' Functional API lets you derive a new model with the layers from an existing model, so you can define the output to be a layer below the output layer in the original model. Calling predict() on this new model will produce the output of that layer for a given input. Of course, glancing at the numbers by themselves doesn't provide any interpretation of what the model has learned (although there are opportunities to [interpret these values](https://www.civisanalytics.com/blog/interpreting-visualizing-neural-networks-text-processing/)), but seeing them verifies the model is just a series of transformations from one matrix to another. The model stores its layers as the list model.layers, and you can retrieve specific layer by its position index in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Show the output of the embedding layer (second layer) for the test reviews'''\n",
    "\n",
    "embedding_layer = Model(inputs=model.layers[0].input, outputs=model.layers[1].output) #embedding layer is 2nd layer (index 1)\n",
    "embedding_output = embedding_layer.predict(test_padded_idxs)\n",
    "print(\"EMBEDDING LAYER OUTPUT SHAPE:\", embedding_output.shape)\n",
    "print(embedding_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also easy to look at the weight matrices that connect the layers. The get_weights() function will show the incoming weights for a particular layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Show weights that connect the hidden layer to the output layer'''\n",
    "\n",
    "hidden_to_output_weights = model.layers[-1].get_weights()[0]\n",
    "print(\"HIDDEN-TO_OUTPUT WEIGHTS SHAPE:\", hidden_to_output_weights.shape)\n",
    "print(hidden_to_output_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "As mentioned above, the model shown here could be applied to any task where the goal is to predict a score for a particular sequence. For ratings prediction, this score is ordinal, but it could also be categorical with a few simple changes to the output layer of the model. My other notebooks for [language modeling/generation](https://github.com/roemmele/keras-rnn-demo/language-modeling) and [part-of-speech tagging](https://github.com/roemmele/keras-rnn-demo/pos-tagging) demonstrate this type of prediction with categorical variables. They also show how to build an RNN in Keras when the output is a sequence of labels, rather than a single value as shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color='#6629b2'>More resources</font>\n",
    "\n",
    "Yoav Goldberg's book [Neural Network Methods for Natural Language Processing](http://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037) is a thorough introduction to neural networks for NLP tasks in general.\n",
    "\n",
    "If you'd like to learn more about what Keras is doing under the hood, there is a [Theano tutorial](http://deeplearning.net/tutorial/lstm.html) that also applies an RNN to sentiment prediction, using the same dataset here\n",
    "\n",
    "Andrej Karpathy's blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is very helpful for understanding the mathematical details of an RNN, applied to the task of language modeling. It also provides raw Python code with an implementation of the backpropagation algorithm.\n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "Chris Olah provides a good [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTM RNNs work (this explanation also applies to the GRU model used here)\n",
    "\n",
    "Denny Britz's [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) documents well both the technical details of RNNs and their implementation in Python."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
